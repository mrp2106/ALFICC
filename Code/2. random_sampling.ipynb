{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f94904a-74fc-482e-bd43-d113fcabc273",
   "metadata": {},
   "source": [
    "# Top\n",
    "\n",
    "* [Setup](#Setup)\n",
    "* [Get Random Samples (with training features)](#Get-Random-Samples-(with-training-features))\n",
    "* [Agglomerate annotated subfiles to create RS dataset](#Agglomerate-annotated-subfiles-to-create-RS-dataset)\n",
    "* [Plot Random Sampling labeled data](#Plot-Random-Sampling-labeled-data)\n",
    "* [Statistics of RS labeled set](#Statistics-of-RS-labeled-set)\n",
    "\n",
    "[Bottom](#Bottom)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2812783f-bbf6-4014-846b-d11a53a99dc0",
   "metadata": {},
   "source": [
    "______________________\n",
    "\n",
    "# Setup\n",
    "\n",
    "[Back to Top](#Top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a70bf61-b9a2-4a61-803e-3ad64718cafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "##/////////////////////////////////////\n",
    "## Imports and Util\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "from tabulate import tabulate\n",
    "\n",
    "import geopandas as gpd\n",
    "import rioxarray\n",
    "import rasterio as rio\n",
    "from shapely.geometry import Point, box\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedGroupKFold, cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score, accuracy_score, balanced_accuracy_score\n",
    "\n",
    "from joblib import load\n",
    "import os\n",
    "import re\n",
    "\n",
    "import random\n",
    "\n",
    "#visualization parameters\n",
    "palette = ['#006600', '#99ff33', '#2d8659', '#c6538c', '#808000', '#804000', '#0000ff']\n",
    "class_names = ['Cashew', 'Non-cashew']\n",
    "vis_params = {\n",
    "        \"cmap\": ListedColormap(palette),\n",
    "        \"vmin\": 1,\n",
    "        \"vmax\": 2,\n",
    "        \"alpha\": 0.9,\n",
    "}\n",
    "\n",
    "base_directory = os.getcwd()\n",
    "#dataset with min max band values for normalization\n",
    "normalizer_df = os.path.join(base_directory, \"\")\n",
    "norm = pd.read_csv(normalizer_df)\n",
    "norm = norm[[\"band\",\"mean\",\"std\",\"min\",\"max\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69822f55-17ae-49f5-b1b0-e347e5017dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "##/////////////////////////////////////\n",
    "## Auxiliary functions to the important ones\n",
    "\n",
    "def get_normalized_X(X, norm):\n",
    "    \"\"\"\n",
    "    Apply min-max normalization to feature dataframe. Returns norm dataframe\n",
    "    \"\"\"\n",
    "    X_norm = X.copy(deep=True)\n",
    "    for band in norm[\"band\"]:\n",
    "        norm_params = norm.loc[norm[\"band\"]==band]\n",
    "        X_norm[band] = (X_norm[band] - norm_params[\"min\"].iloc[0]) / (norm_params[\"max\"].iloc[0] - norm_params[\"min\"].iloc[0]) #iloc[0] because of FutureWarning\n",
    "    return X_norm\n",
    "\n",
    "##############################################\n",
    "\n",
    "def transform_y_2classes(y):\n",
    "    \"\"\"\n",
    "    Transforms a y labeled array/Series from the 7 classes, where 5 is cashew, into a 2 labeled Series where 1 cashew, 2 non-cashew\n",
    "    \"\"\"\n",
    "    y_update = pd.Series(y == 5, dtype=\"int\")\n",
    "    y_update.loc[y_update==0] = 2\n",
    "    return y_update\n",
    "\n",
    "##############################################\n",
    "\n",
    "def balanced_accuracy_scorer(estimator, X_true, y_true):\n",
    "    y_pred = estimator.predict(X_true)\n",
    "    if (np.unique(y_pred) != np.unique(y_true)).all():\n",
    "        print(\"Classes in true labels and predictions mismatch!\")\n",
    "        print(\"Unique preds: \", np.unique(y_pred))\n",
    "        print(\"Unique trues\", np.unique(y_true))\n",
    "    return balanced_accuracy_score(y_true,y_pred)\n",
    "\n",
    "def f1_cashew_scorer(estimator, X_true, y_true):\n",
    "    \"\"\"\n",
    "    Return F1 score for cashew. Adapted to the 7-class system where cashew is class 5, or to the binary system where cashew is class 1\n",
    "    \"\"\"\n",
    "    y_pred = estimator.predict(X_true)\n",
    "    if (np.unique(y_pred) != np.unique(y_true)).all():\n",
    "        print(\"Classes in true labels and predictions mismatch!\")\n",
    "        print(\"Unique preds: \", np.unique(y_pred))\n",
    "        print(\"Unique trues\", np.unique(y_true))\n",
    "    if np.unique(y_true).shape[0] == 2:\n",
    "        return f1_score(y_true,y_pred,average=None)[0] #assumes 1st class is cashew, 2nd is non      \n",
    "    else:\n",
    "        try:\n",
    "            return f1_score(y_true,y_pred,average=None)[4] #in the 7 total classes, cashew is 5th\n",
    "        except IndexError: #other number of classes, could be just 1, if cashew doesn't appear for example\n",
    "            return -1\n",
    "\n",
    "##############################################\n",
    "\n",
    "def plot_multipolygon_boundaries(multi,color=\"black\",linewidth=0.2,alpha=1):\n",
    "    \"\"\"\n",
    "    Works with polygons or multipolygons, plots just the outlines of the shape\n",
    "    \"\"\"\n",
    "    for geom in multi.geoms:\n",
    "        xs, ys = geom.exterior.xy    \n",
    "        plt.plot(xs,ys, color=color, linewidth=linewidth,alpha=alpha)\n",
    "\n",
    "##############################################\n",
    "\n",
    "def filterOverlappingPoints(df1, df2):\n",
    "    \"\"\"\n",
    "    Returns: df1 with just the rows that do not appear in df2, according to xy position\n",
    "    \"\"\"\n",
    "    if df2.empty == False:\n",
    "        merged_df = pd.merge(df1, df2, on=[\"x\",\"y\"], how=\"left\", indicator=True)\n",
    "        return merged_df[merged_df[\"_merge\"]==\"left_only\"].index\n",
    "    else:\n",
    "        return df1.index\n",
    "\n",
    "##############################################\n",
    "\n",
    "def find_latest_file(directory):\n",
    "    '''\n",
    "    Get directory+filename of a file in a directory with highest number.\n",
    "    Filenames follow the form: someName_nº.extension\n",
    "    Generalization of savePickle.py's find_latest_pickle()\n",
    "    '''\n",
    "    files = [file for file in os.listdir(directory)]\n",
    "    if not files:\n",
    "        return None, -1 \n",
    "\n",
    "    #extract suffix numbers from file names and find the maximum\n",
    "    suffix_numbers = [int(file.split(\"_\")[-1].split(\".\")[0]) for file in files]\n",
    "    index_of_max = suffix_numbers.index(max(suffix_numbers))\n",
    "    \n",
    "    return os.path.join(directory, files[index_of_max]), suffix_numbers[index_of_max]\n",
    "\n",
    "##############################################\n",
    "\n",
    "def find_n_lowest_difference_index(df, n_points):\n",
    "    \"\"\"\n",
    "    Given a pandas dataframe df with column \"Difference\", return the indexes of points with lowest \"Difference\" between first and second classes  \n",
    "    \"\"\"\n",
    "    df[\"Difference\"] = -df[\"Difference\"] #to utilize numpy's nlargest, invert sign\n",
    "    largest = df.nlargest(n_points,\"Difference\")\n",
    "    largest[\"Difference\"] = -largest[\"Difference\"] #re-revert sign\n",
    "    return largest.index;\n",
    "\n",
    "###############################################\n",
    "\n",
    "def get_files_in_directory(directory_path):\n",
    "    \"\"\"\n",
    "    Returns: list with all files in the specified directory\n",
    "    \"\"\"\n",
    "    files = os.listdir(directory_path)\n",
    "    #filter out directories, leaving only files\n",
    "    files = [file for file in files if os.path.isfile(os.path.join(directory_path, file))]\n",
    "    return files\n",
    "\n",
    "###############################################\n",
    "\n",
    "def extract_number_from_filename(file_name):\n",
    "    \"\"\"\n",
    "    Extracts the number from a file name before the extension (if any).\n",
    "    \"\"\"\n",
    "    match = re.search(r'(\\d+)(\\.\\w+)?$', file_name)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579091d4-3ccd-4304-a46c-39332f5c5282",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "# Get Random Samples (with training features)\n",
    "\n",
    "[Back to top](#Top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3c7095-8ad5-45b7-ae9d-b28f73dbdafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import rasterio\n",
    "import pandas as pd\n",
    "random.seed(42)\n",
    "\n",
    "def geotiff_to_df(image_path, column_order=None, standardize=False, norm_dataframe=None):\n",
    "    \"\"\"\n",
    "    Converts a geotiff file into a pandas dataframe.\n",
    "    CORRECTED v3 - changed standardization to be global (using min max parameters from the entire region of interest) instead of batch-based normalization.\n",
    "    Assumes the images are on crs \"EPSG:4326\".\n",
    "    column_order: the order of bands in xds is different than the band order in model training. This parameter is to give the correct training column order.\n",
    "    \"\"\"\n",
    "    xds = rioxarray.open_rasterio(image_path, masked=True)\n",
    "    x_coords, y_coords = xds.coords['x'].values, xds.coords['y'].values\n",
    "\n",
    "    #for geotiffs with only one band, tipically the class band, simpler transformation, without normalization\n",
    "    if xds.shape[0] == 1:\n",
    "        values = xds[0].values.flatten()\n",
    "        df = pd.DataFrame({'x': list(x_coords)*len(y_coords),\n",
    "                           'y': y_coords.repeat(len(x_coords)),\n",
    "                           'band_0': values})\n",
    "        return df\n",
    "\n",
    "    names = xds.attrs[\"long_name\"]\n",
    "    df_final = pd.DataFrame(columns=xds.attrs[\"long_name\"])\n",
    "    \n",
    "    #correct column order if needed\n",
    "    if column_order is not None:\n",
    "        df_final = df_final[column_order]\n",
    "    #if standardize, flatten the 2D/3D array and apply min-max norm; else, just flatten the array\n",
    "    if standardize:\n",
    "        for i, name in enumerate(names):\n",
    "            values = xds[i].values.flatten() #each band has format (y_size, x_size); flatten() transform into 1d array with size (y_size * x_size)\n",
    "            norm_params = norm_dataframe.loc[norm[\"band\"]==name]\n",
    "            df_final[name] = (values - norm_params[\"min\"].iloc[0]) / (norm_params[\"max\"].iloc[0] - norm_params[\"min\"].iloc[0]) #iloc[0] due to FutureWarning; though pd.Series only has 1 element\n",
    "    else:\n",
    "        for i, name in enumerate(names):\n",
    "            values = xds[i].values.flatten()\n",
    "            df_final[name] = values\n",
    "\n",
    "    #merge lon and lat columns\n",
    "    if \"x\" not in df_final.columns:\n",
    "        df_final[\"x\"] = list(x_coords)*len(y_coords)\n",
    "        df_final[\"y\"] = y_coords.repeat(len(x_coords))\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "###############################################\n",
    "\n",
    "def obtain_patch_coordinates(tif_to_df_unique_x, tif_to_df_unique_y, point_coordinates, polygonID, verbose):\n",
    "    \"\"\"\n",
    "    Given coordinates for a point (central pixel), returns a dataframe with the coordinates for the direct 8 neighboring pixels. Works for a list of point_coordinates.\n",
    "    tif_to_df_unique_x and y regard the possible raster x and y coordinates.\n",
    "    polygonID is given to keep track of the unique patches. Every 3x3 pixel patch has a unique polygonID for identification and data split purposes.\n",
    "    \"\"\"\n",
    "    final_x = []\n",
    "    final_y = []\n",
    "    final_polygonID = []\n",
    "    for point in zip(point_coordinates[\"x\"], point_coordinates[\"y\"]):\n",
    "        x_indices = []\n",
    "        y_indices = []\n",
    "        central_index_x = np.where(tif_to_df_unique_x == point[0])[0][0] #double [0] to obtain the index\n",
    "        central_index_y = np.where(tif_to_df_unique_y == point[1])[0][0]\n",
    "        x_indices.append(tif_to_df_unique_x[central_index_x])\n",
    "        y_indices.append(tif_to_df_unique_y[central_index_y])\n",
    "        #evaluates if the central pixel is on any border of the raster. Special cases if it is, otherwise adds the 8 neighboring pixels.\n",
    "        if central_index_x == 0:\n",
    "            if verbose:\n",
    "                print(\"entered 1\")\n",
    "            x_indices.append(tif_to_df_unique_x[central_index_x+1])\n",
    "        elif central_index_x == (tif_to_df_unique_x.shape[0]-1):\n",
    "            if verbose: \n",
    "                print(\"entered 2\")\n",
    "            x_indices.append(tif_to_df_unique_x[central_index_x-1])\n",
    "        else:\n",
    "            x_indices.append(tif_to_df_unique_x[central_index_x+1])\n",
    "            x_indices.append(tif_to_df_unique_x[central_index_x-1])\n",
    "        if central_index_y == 0:\n",
    "            if verbose:\n",
    "                print(\"entered 4\")\n",
    "            y_indices.append(tif_to_df_unique_y[central_index_y+1])\n",
    "        elif central_index_y == (tif_to_df_unique_y.shape[0]-1):\n",
    "            if verbose:\n",
    "                print(\"entered 5\")\n",
    "            y_indices.append(tif_to_df_unique_y[central_index_y-1])\n",
    "        else:\n",
    "            y_indices.append(tif_to_df_unique_y[central_index_y+1])\n",
    "            y_indices.append(tif_to_df_unique_y[central_index_y-1]) \n",
    "        xy_coords = np.array(np.meshgrid(x_indices, y_indices)).T.reshape(-1,2) #from https://stackoverflow.com/questions/1208118/using-numpy-to-build-an-array-of-all-combinations-of-two-arrays\n",
    "        final_x.append(xy_coords[:,0])\n",
    "        final_y.append(xy_coords[:,1])\n",
    "        final_polygonID.append([polygonID for i in range(xy_coords.shape[0])])\n",
    "        polygonID += 1\n",
    "\n",
    "    df_return = pd.DataFrame({\"x\": np.concatenate(final_x), \"y\": np.concatenate(final_y), \"polygonID\": np.concatenate(final_polygonID)})\n",
    "    return df_return, polygonID #polygonID is also returned to keep track of the change in unique patches\n",
    "\n",
    "###############################################\n",
    "\n",
    "def random_coefs_from_geotiffs(geotiffs_path, num_points_per_geotiff=1000, num_indexes_to_return=None, check_inside_multipolygon=True,\n",
    "                               column_order=None, standardize=None, norm_dataframe=None, random_seed=0, polygonID=1000):\n",
    "    \"\"\"\n",
    "    Randomly selects points from geotiff files in the specified directory.\n",
    "    directory_path (str): path to the directory containing the geotiff files.\n",
    "    num_points_per_geotiff (int): number of points to randomly select from each geotiff.\n",
    "    num_indexes_to_return (int): number of randomly selected indexes to return.\n",
    "    Returns:\n",
    "    dataframe with columns \"x\", \"y\", and \"geotiff\" indicating \n",
    "    the coordinates of the randomly selected points and the original geotiff file name.\n",
    "    \"\"\"\n",
    "    final_df = pd.DataFrame()\n",
    "    random.seed(random_seed) #stipulate seed of random sample selection for reproducibility\n",
    "    for root, dirs, files in os.walk(geotiffs_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".tif\") or file.endswith(\".tiff\"):  #check if file is a GeoTIFF\n",
    "                file_path = os.path.join(root, file)\n",
    "                print(f\"Processing GeoTIFF: {file}\")\n",
    "                #transform geotiff into dataframe\n",
    "                tif_to_df = geotiff_to_df(file_path)\n",
    "                #sample a number of CENTRAL PIXELS\n",
    "                selected_indexes = random.sample(range(tif_to_df.shape[0]), num_points_per_geotiff)\n",
    "                tif_to_df_selected = tif_to_df.iloc[selected_indexes]\n",
    "                #also get the neighboring pixels of each selected central pixel (tipically working with just the 8 direct neighboring pixels) \n",
    "                xypatches, polygonID = obtain_patch_coordinates(np.unique(tif_to_df[\"x\"]), np.unique(tif_to_df[\"y\"]), tif_to_df_selected, polygonID)\n",
    "                tif_to_df_patches = pd.merge(tif_to_df, xypatches[[\"x\",\"y\",\"polygonID\"]], on=[\"x\",\"y\"], how=\"inner\")\n",
    "                tif_to_df_patches[\"geotiff\"] = extract_number_from_filename(file)\n",
    "                #add pixels (central + neighbors) to final_df\n",
    "                if final_df.empty:\n",
    "                    final_df = tif_to_df_patches\n",
    "                else:\n",
    "                    final_df = pd.concat([final_df, tif_to_df_patches], ignore_index=True)\n",
    "                print(\"OG tif_df file size: \", tif_to_df.shape)\n",
    "                print(\"selected indices: \", tif_to_df_selected.shape)\n",
    "                print(\"Size with patches: \", tif_to_df_patches.shape)\n",
    "                print(\"polygonID: \", polygonID)\n",
    "\n",
    "    #safety check if pixels are inside ROI multipolygon\n",
    "    if check_inside_multipolygon:\n",
    "        final_df.reset_index(drop=True,inplace=True)\n",
    "        print(\"Before inside_multi_polygon: \", final_df.shape)\n",
    "        inside_multi_polygon = [point_inside_multi_polygon(x, y, multi_polygon) for x, y in zip(final_df[\"x\"], final_df[\"y\"])]\n",
    "        final_df = final_df.loc[inside_multi_polygon]\n",
    "        print(\"After inside_multi_polygon: \", final_df.shape)\n",
    "    \n",
    "    #final cut of points from final_df, if shape is still higher than the desired number of samples to annotate\n",
    "    final_df.reset_index(drop=True,inplace=True)\n",
    "    if num_indexes_to_return is not None:\n",
    "        print(\"num_indexes_to_return is not None, cutting until desired number\")\n",
    "        selected_indexes = random.sample(range(len(final_df)), num_indexes_to_return)\n",
    "        final_df = final_df.iloc[selected_indexes]\n",
    "        final_df.reset_index(drop=True,inplace=True)\n",
    "        \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a98da5-61e4-4361-bfb9-aed4df8a6ead",
   "metadata": {},
   "source": [
    "* __Obtain randomly selected patches (before checking inside multipolygon)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775f6105-05df-4404-b976-20f707420f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "geojson_file = \"\"\n",
    "gdf = gpd.read_file(geojson_file)\n",
    "\n",
    "multi_polygon = gdf.unary_union\n",
    "\n",
    "base_directory = os.getcwd()\n",
    "geotiffs_path = os.path.join(base_directory, \"\")\n",
    "random_path = os.path.join(base_directory, \"\")\n",
    "\n",
    "num_points_per_geotiff = 10\n",
    "num_indexes_to_return = None\n",
    "random_points_df = random_coefs_from_geotiffs(geotiffs_path, num_points_per_geotiff, num_indexes_to_return, random_seed=0, polygonID = 1000, check_inside_multipolygon=True)\n",
    "print(\"Randomly selected points DataFrame:\")\n",
    "print(random_points_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e9b80e-67ba-4424-addb-b88ab678d608",
   "metadata": {},
   "source": [
    "* __Split dataframe into multiple smaller dataframes__\n",
    "\n",
    "__Disclaimer:__ This step wasn't strictly necessary. However, the randomly selected data samples will be imported into Google Earth Pro, which doesn't work well with large datasets. So, partitioning helps importing sample fractions at a time to the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4a031d-4755-4648-8c3d-764a98c1a40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataframe_by_polygon(df, num_subfiles, output_folder=None, output_folder_gep=None):\n",
    "    \"\"\"\n",
    "    Shuffle polygonID's, and get random patches for each sub-file.\n",
    "    Subfiles are stores in two different folders:\n",
    "    output_folder_gep, where files will be imported into Google Earth Pro for the class annotation process and will thus be edited;\n",
    "    output_folder, safety save where files will not be edited\n",
    "    \"\"\"\n",
    "    #create output folders if needed\n",
    "    if output_folder != None:\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "    if output_folder_gep != None:\n",
    "        os.makedirs(output_folder_gep, exist_ok=True)\n",
    "\n",
    "    #shuffle randomly selected patches (uniquely identified by polygonID) and split them throughout a number of sub-files\n",
    "    unique_polygon_ids = df['polygonID'].unique()\n",
    "    random.shuffle(unique_polygon_ids)    \n",
    "    samples_per_subfile = len(unique_polygon_ids) // num_subfiles\n",
    "    for i in range(num_subfiles):\n",
    "        #calculate start and end indices for slicing\n",
    "        start_index = i * samples_per_subfile\n",
    "        end_index = start_index + samples_per_subfile\n",
    "        if i == num_subfiles - 1: #if it's the last subfile, include remaining IDs\n",
    "            end_index = None\n",
    "        \n",
    "        subset_polygon_ids = unique_polygon_ids[start_index:end_index]\n",
    "        sub_df = df[df['polygonID'].isin(subset_polygon_ids)]\n",
    "        sub_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        subfile_path = os.path.join(output_folder, f'randomsub_{i+1}.csv')\n",
    "        sub_df.to_csv(subfile_path)\n",
    "        subfile_path = os.path.join(output_folder_gep, f'randomsub_{i+1}.csv')\n",
    "        sub_df[[\"x\", \"y\", \"polygonID\", \"geotiff\"]].to_csv(subfile_path)\n",
    "\n",
    "    print(f\"{num_subfiles} sub-files created successfully in '{output_folder}' folder.\")\n",
    "    print(f\"{num_subfiles} sub-files created successfully in '{output_folder_gep}' folder.\")\n",
    "    return;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4d8175-a48e-426e-890e-b1bb6631aa26",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = \"\"\n",
    "output_folder_gep = \"\"\n",
    "test_df = split_dataframe_by_polygon(random_points_df_inside, 30, output_folder=output_folder, output_folder_gep=output_folder_gep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6233bd75-ae86-4c1d-a1b8-5f94c345196f",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "# Agglomerate annotated subfiles to create RS dataset\n",
    "\n",
    "[Back to top](#Top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bf09ec-d613-41b0-be51-5655e18af25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_with_classes = os.path.join(base_directory, \"\") #directory of GEP imported files, where we added a Class column\n",
    "directory_with_coefs = os.path.join(base_directory, \"\") #directory with the dataframes with training coefficients\n",
    "\n",
    "agg_df = pd.DataFrame()\n",
    "\n",
    "for file in os.listdir(directory_with_classes):\n",
    "    file_classes = os.path.join(directory_with_classes, file)\n",
    "    classes_df = pd.read_csv(file_classes, encoding=\"latin-1\") #due to special characters like \"?\"\n",
    "    #if dataset has no \"Class\" column, we haven't labeled this sub-file yet, ignore it\n",
    "    if \"Class\" not in classes_df.columns:\n",
    "        continue\n",
    "    if (classes_df[\"Unnamed: 0\"] == classes_df.index).all() == False: #order rows has been altered, revert it\n",
    "        classes_df.sort_values(by=\"Unnamed: 0\", axis=0, inplace=True, ignore_index=True)\n",
    "    \n",
    "    #add class and class certainty columns to coefs_df\n",
    "    file_coefs = os.path.join(directory_with_coefs, file)\n",
    "    coefs_df = pd.read_csv(file_coefs)\n",
    "    coefs_df[\"Class\"] = classes_df[\"Class\"]\n",
    "    coefs_df[\"Pred certainty\"] = classes_df[\"Pred certainty\"]\n",
    "\n",
    "    #concatenate to final manually labeled dataset\n",
    "    agg_df = pd.concat([agg_df, coefs_df], ignore_index=True)\n",
    "agg_df.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "#save fully labeled Random Sampling dataset!!!\n",
    "rs_save_file=os.path.join(base_directory, \"\")\n",
    "agg_df.to_pickle(rs_save_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5574efb7-b697-42a6-b754-2815ab6dd6c1",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "## __Plot Random Sampling labeled data__\n",
    "\n",
    "[Back to top](#Top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf00b82-f7b4-44b1-8f4f-673c1db2a688",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_df = pd.read_pickle(rs_save_file)\n",
    "\n",
    "#samples that were observed but could not be attributed a class were labeled with -1. Clean dataset of those samples\n",
    "clean_rs_df = rs_df[rs_df[\"Class\"] > 0]\n",
    "clean_rs_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "roi_directory = os.path.join(base_directory, \"\")\n",
    "region_of_interest = gpd.read_file(roi_directory)\n",
    "multipolygon = region_of_interest.unary_union #for plotting, working with just a single multipolygon is simpler\n",
    "\n",
    "s2cc_sklearn_preds_file = os.path.join(base_directory, \"\") #first predictions land cover map we produced\n",
    "\n",
    "print(\"Total dataset size: \", rs_df.shape)\n",
    "print(\"Class>0 dataset size: \", rs_df.loc[rs_df[\"Class\"]>0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf4bc98-8d33-41c0-b4e2-55644e227b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualization parameters\n",
    "colors = {1: '#006600', 2: '#99ff33', 3: \"#2d8659\", 4: '#c6538c', 5: '#808000', 6: '#804000', 7: '#0000ff', 8:'orange'}\n",
    "palette = ['#006600', '#99ff33', '#2d8659', '#c6538c', '#808000', '#804000', '#0000ff', 'orange']\n",
    "class_names = ['Closed Forest', 'Open Forest', 'Mangrove', 'Savanna', 'Cashew', 'Non-Forest', 'Water', 'Clearly non-cashew']\n",
    "vis_params = {\n",
    "    \"cmap\": ListedColormap(palette),\n",
    "    \"vmin\": 1,\n",
    "    \"vmax\": 7,\n",
    "    \"alpha\": 0.9,\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(18,6))\n",
    "\n",
    "#plot every Random Sampling sample, even those with Class -1 (that could not be attributed a land cover class)\n",
    "plt.subplot(1,2,1)\n",
    "plot_multipolygon_boundaries(multipolygon,color=\"black\",linewidth=0.6)\n",
    "for al_class, color in colors.items():\n",
    "    subset = rs_df[rs_df['Class'] == al_class]\n",
    "    plt.scatter(subset['x'], subset['y'], s=10, color=color, label=class_names[al_class-1])\n",
    "subset = rs_df[rs_df['Class'] < 0]\n",
    "plt.scatter(subset['x'], subset['y'], s=10, color=\"black\", label=\"Not classified\")\n",
    "plt.title('AL samples - Classes')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.legend()\n",
    "\n",
    "#plot the CLEAN Random Sampling dataset, without the Class -1 samples\n",
    "plt.subplot(1,2,2)\n",
    "plot_multipolygon_boundaries(multipolygon,color=\"black\",linewidth=0.6)\n",
    "for al_class, color in colors.items():\n",
    "    subset = clean_rs_df[clean_rs_df['Class'] == al_class]\n",
    "    plt.scatter(subset['x'], subset['y'], s=10, color=color, label=class_names[al_class-1])\n",
    "plt.title('CLEAN AL samples - Classes')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b8f7d4-cc6a-4d79-b34d-9fcac2d9367a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#similar to previous notebook cell, but instead of plotting every class, just plots points according to whether they were attributed a class (Class > 0) or not (Class = -1)\n",
    "\n",
    "plt.figure(figsize=(18,6))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plot_multipolygon_boundaries(multipolygon,color=\"black\",linewidth=0.6)\n",
    "subset = rs_df[rs_df['Class'] >= 0]\n",
    "plt.scatter(subset['x'], subset['y'], s=10, color=\"green\", label=\"Classified\")\n",
    "subset = rs_df[rs_df['Class'] < 0]\n",
    "plt.scatter(subset['x'], subset['y'], s=10, color=\"red\", label=\"Not classified\")\n",
    "plt.title('AL samples - Classified vs. not classified')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plot_multipolygon_boundaries(multipolygon,color=\"black\",linewidth=0.6)\n",
    "subset = rs_df[rs_df['Class'] >= 0]\n",
    "plt.scatter(subset['x'], subset['y'], s=10, color=\"green\", label=\"Classified\")\n",
    "plt.title('AL samples - Just Classified')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d8137c-fae6-4131-bd99-52b2e2550ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the land cover map with overlay of the Random Sampling dataset\n",
    "\n",
    "with rasterio.open(s2cc_sklearn_preds_file) as src:\n",
    "    image = src.read(1)\n",
    "\n",
    "masked_image = np.ma.masked_outside(image, 1, 7) #set values outside the range [1, 7] to be transparent\n",
    "\n",
    "#plot land cover map\n",
    "\n",
    "palette = ['#006600', '#99ff33', '#2d8659', '#c6538c', '#808000', '#804000', '#0000ff']\n",
    "class_names = ['Closed Forest', 'Open Forest', 'Mangrove', 'Savanna', 'Cashew', 'Non-Forest', 'Water']\n",
    "vis_params = {\n",
    "    \"cmap\": ListedColormap(palette),\n",
    "    \"vmin\": 1,\n",
    "    \"vmax\": 7,\n",
    "    \"alpha\": 0.5,\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(15,12), facecolor='white') #white background color\n",
    "left, bottom, right, top = src.bounds\n",
    "img = plt.imshow(masked_image, **vis_params, extent=(left, right, bottom, top))\n",
    "\n",
    "cbar = plt.colorbar(img, ticks=np.arange(1, 8), label=\"Class\", orientation=\"vertical\", shrink=0.4)\n",
    "cbar.set_ticklabels(class_names)\n",
    "plt.title(\"Land cover map 2021\")\n",
    "plt.axis(\"off\")\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "\n",
    "#plot boundaries and random sampling points\n",
    "\n",
    "plot_multipolygon_boundaries(multipolygon,color=\"black\",linewidth=0.6)\n",
    "\n",
    "colors = {1: '#006600', 2: '#99ff33', 3: \"#2d8659\", 4: '#c6538c', 5: '#808000', 6: '#804000', 7: '#0000ff', 8: 'orange'}\n",
    "class_names = ['Closed Forest', 'Open Forest', 'Mangrove', 'Savanna', 'Cashew', 'Non-Forest', 'Water', 'Clearly not cashew']\n",
    "for al_class, color in colors.items():\n",
    "    subset = rs_df[rs_df['Class'] == al_class]\n",
    "    plt.scatter(subset['x'], subset['y'], s=20, color=color, label=class_names[al_class-1], linewidths=0.5, edgecolors=\"black\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ead198-0476-4adb-9aef-0c55746a3b3b",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "# Statistics of RS labeled set\n",
    "\n",
    "[Back to top](#Top)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ca040c-7937-4110-aed6-cc981cc00e32",
   "metadata": {},
   "source": [
    "* __PolygonID sizes (nº of samples per polygon)__\n",
    "\n",
    "24 of 130 not fully annotated patches (nºpixels < 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178e4960-969c-48d4-a693-a603bf43fe60",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_counts = clean_rs_df[\"polygonID\"].value_counts()\n",
    "unique_counts = group_counts.value_counts()\n",
    "\n",
    "plt.bar(unique_counts.index, unique_counts.values)\n",
    "plt.title(\"Barplot of nº pixels per patch\")\n",
    "plt.xlabel(\"Nº pixels per patch\")\n",
    "plt.ylabel(\"Patch count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cbd62f-8eb0-417a-a4a8-180a6928df60",
   "metadata": {},
   "source": [
    "* __Pixel counts per class__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25ad364-bd99-4382-a95b-63a25105c077",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "colors = {-1: \"black\", 1: '#006600', 2: '#99ff33', 3: \"#2d8659\", 4: '#c6538c', 5: '#808000', 6: '#804000', 7: '#0000ff', 8:'orange'}\n",
    "\n",
    "#counts for entire RS dataset (with Class -1 points)\n",
    "plt.subplot(1,2,1)\n",
    "\n",
    "class_counts = rs_df['Class'].value_counts()\n",
    "class_counts = class_counts.sort_index()\n",
    "class_counts.plot(kind='bar', color=[colors[cls] for cls in class_counts.index])\n",
    "\n",
    "plt.title('Class counts - entire RS dataset')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "#counts for CLEAN RS dataset (without Class -1 points)\n",
    "plt.subplot(1,2,2)\n",
    "\n",
    "class_counts = clean_rs_df['Class'].value_counts()\n",
    "class_counts = class_counts.sort_index()\n",
    "class_counts.plot(kind='bar', color=[colors[cls] for cls in class_counts.index])\n",
    "\n",
    "plt.title('Class counts - CLEAN RS dataset')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5873a581-07a5-404f-93b6-5b70b8947314",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "# Bottom\n",
    "\n",
    "[Back to Top.](#Top)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
