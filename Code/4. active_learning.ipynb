{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46d9e5f0-4a42-405d-9859-fa5d649a4c95",
   "metadata": {},
   "source": [
    "# Top\n",
    "\n",
    "1. __Imports__ and auxiliary scripts - [here](#Imports-and-auxiliar-scripts)\n",
    "2. Run __BvSB__, and __store__ dataset to classify\n",
    "    * Auxiliary functions - [here](#Run-BvSB,-and-store-dataset-to-classify)\n",
    "    * Runnable cells - [here](#BvSVB-get-and-store-dataset---runnable)\n",
    "3. __Update__ datasets and trained model\n",
    "    * Auxiliary functions - [here](#Update-datasets-and-trained-model)\n",
    "    * Runnable cells - [here](#Update-datasets-and-trained-model---runnable)\n",
    "4. AL iter performance - [here](#AL-iter-performance)\n",
    "    * [Stats per iteration](#Stats-per-iteration)\n",
    "\n",
    "[Bottom](#Bottom)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b169ad3a-3004-4d01-ade1-5ffc8460ef88",
   "metadata": {},
   "source": [
    "______________\n",
    "\n",
    "# Imports and auxiliar scripts\n",
    "\n",
    "[Back to top](#Top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd4f086-037d-4be0-8ff6-2d82720b348a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from tabulate import tabulate\n",
    "\n",
    "import os\n",
    "from joblib import load\n",
    "import re\n",
    "\n",
    "import geopandas as gpd\n",
    "import rioxarray\n",
    "import rasterio as rio\n",
    "from shapely.geometry import Point, box\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedGroupKFold, cross_val_score\n",
    "from sklearn.metrics import f1_score, accuracy_score, balanced_accuracy_score\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "base_directory = os.getcwd()\n",
    "\n",
    "normalizer_df_directory = os.path.join(base_directory, \"\")\n",
    "norm = pd.read_csv(normalizer_df_directory)\n",
    "norm = norm[[\"band\",\"mean\",\"std\",\"min\",\"max\"]]\n",
    "\n",
    "palette = ['#006600', '#99ff33', '#2d8659', '#c6538c', '#808000', '#804000', '#0000ff']\n",
    "class_names = ['Cashew', 'Non-cashew']\n",
    "vis_params = {\n",
    "        \"cmap\": ListedColormap(palette),\n",
    "        \"vmin\": 1,\n",
    "        \"vmax\": 2,\n",
    "        \"alpha\": 0.9,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef8f558-f358-43b4-a10c-50d6f1c20e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normalized_X(X, norm):\n",
    "    \"\"\"\n",
    "    Apply min-max normalization to feature dataframe. Returns norm dataframe\n",
    "    \"\"\"\n",
    "    X_norm = X.copy(deep=True)\n",
    "    for band in norm[\"band\"]:\n",
    "        norm_params = norm.loc[norm[\"band\"]==band]\n",
    "        X_norm[band] = (X_norm[band] - norm_params[\"min\"].iloc[0]) / (norm_params[\"max\"].iloc[0] - norm_params[\"min\"].iloc[0]) #iloc[0] because of FutureWarning\n",
    "    return X_norm\n",
    "\n",
    "##############################################\n",
    "\n",
    "def transform_y_2classes(y):\n",
    "    \"\"\"\n",
    "    Transforms a y labeled array/Series from the 7 classes, where 5 is cashew, into a 2 labeled Series where 1 cashew, 2 non-cashew\n",
    "    \"\"\"\n",
    "    y_update = pd.Series(y == 5, dtype=\"int\")\n",
    "    y_update.loc[y_update==0] = 2\n",
    "    return y_update\n",
    "\n",
    "##############################################\n",
    "\n",
    "def balanced_accuracy_scorer(estimator, X_true, y_true):\n",
    "    y_pred = estimator.predict(X_true)\n",
    "    if (np.unique(y_pred) != np.unique(y_true)).all():\n",
    "        print(\"Classes in true labels and predictions mismatch!\")\n",
    "        print(\"Unique preds: \", np.unique(y_pred))\n",
    "        print(\"Unique trues\", np.unique(y_true))\n",
    "    return balanced_accuracy_score(y_true,y_pred)\n",
    "\n",
    "def f1_cashew_scorer(estimator, X_true, y_true):\n",
    "    \"\"\"\n",
    "    Return F1 score for cashew. Adapted to the 7-class system where cashew is class 5, or to the binary system where cashew is class 1\n",
    "    \"\"\"\n",
    "    y_pred = estimator.predict(X_true)\n",
    "    if (np.unique(y_pred) != np.unique(y_true)).all():\n",
    "        print(\"Classes in true labels and predictions mismatch!\")\n",
    "        print(\"Unique preds: \", np.unique(y_pred))\n",
    "        print(\"Unique trues\", np.unique(y_true))\n",
    "    if np.unique(y_true).shape[0] == 2:\n",
    "        return f1_score(y_true,y_pred,average=None)[0] #assumes 1st class is cashew, 2nd is non      \n",
    "    else:\n",
    "        try:\n",
    "            return f1_score(y_true,y_pred,average=None)[4] #in the 7 total classes, cashew is 5th\n",
    "        except IndexError: #other number of classes, could be just 1, if cashew doesn't appear for example\n",
    "            return -1\n",
    "\n",
    "##############################################\n",
    "\n",
    "def plot_multipolygon_boundaries(multi,color=\"black\",linewidth=0.2,alpha=1):\n",
    "    \"\"\"\n",
    "    Works with polygons or multipolygons, plots just the outlines of the shape\n",
    "    \"\"\"\n",
    "    for geom in multi.geoms:\n",
    "        xs, ys = geom.exterior.xy    \n",
    "        plt.plot(xs,ys, color=color, linewidth=linewidth,alpha=alpha)\n",
    "\n",
    "##############################################\n",
    "\n",
    "def filterOverlappingPoints(df1, df2):\n",
    "    \"\"\"\n",
    "    Returns: df1 with just the rows that do not appear in df2, according to xy position\n",
    "    \"\"\"\n",
    "    if df2.empty == False:\n",
    "        merged_df = pd.merge(df1, df2, on=[\"x\",\"y\"], how=\"left\", indicator=True)\n",
    "        return merged_df[merged_df[\"_merge\"]==\"left_only\"].index\n",
    "    else:\n",
    "        return df1.index\n",
    "\n",
    "##############################################\n",
    "\n",
    "def find_latest_file(directory):\n",
    "    '''\n",
    "    Get directory+filename of a file in a directory with highest number.\n",
    "    Filenames follow the form: someName_nº.extension\n",
    "    Generalization of savePickle.py's find_latest_pickle()\n",
    "    '''\n",
    "    files = [file for file in os.listdir(directory)]\n",
    "    if not files:\n",
    "        return None, -1 \n",
    "\n",
    "    #extract suffix numbers from file names and find the maximum\n",
    "    suffix_numbers = [int(file.split(\"_\")[-1].split(\".\")[0]) for file in files]\n",
    "    index_of_max = suffix_numbers.index(max(suffix_numbers))\n",
    "    \n",
    "    return os.path.join(directory, files[index_of_max]), suffix_numbers[index_of_max]\n",
    "\n",
    "##############################################\n",
    "\n",
    "def find_n_lowest_difference_index(df, n_points):\n",
    "    \"\"\"\n",
    "    Given a pandas dataframe df with column \"Difference\", return the indexes of points with lowest \"Difference\" between first and second classes  \n",
    "    \"\"\"\n",
    "    df[\"Difference\"] = -df[\"Difference\"] #to utilize numpy's nlargest, invert sign\n",
    "    largest = df.nlargest(n_points,\"Difference\")\n",
    "    largest[\"Difference\"] = -largest[\"Difference\"] #re-revert sign\n",
    "    return largest.index;\n",
    "\n",
    "###############################################\n",
    "\n",
    "def get_files_in_directory(directory_path):\n",
    "    \"\"\"\n",
    "    Returns: list with all files in the specified directory\n",
    "    \"\"\"\n",
    "    files = os.listdir(directory_path)\n",
    "    #filter out directories, leaving only files\n",
    "    files = [file for file in files if os.path.isfile(os.path.join(directory_path, file))]\n",
    "    return files\n",
    "\n",
    "###############################################\n",
    "\n",
    "def extract_number_from_filename(file_name):\n",
    "    \"\"\"\n",
    "    Extracts the number from a file name before the extension (if any).\n",
    "    \"\"\"\n",
    "    match = re.search(r'(\\d+)(\\.\\w+)?$', file_name)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b506b2a-36ca-4665-80db-fa96fd8650b2",
   "metadata": {},
   "source": [
    "* __Call and pre-process Random Sampling and RSeT datasets__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82c137c-1ba7-45c6-b2be-751260f4dc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_rs_df = pd.read_pickle(os.path.join(base_directory, \"\"))\n",
    "rs_train = pd.read_pickle(os.path.join(base_directory, \"\"))\n",
    "rs_test = pd.read_pickle(os.path.join(base_directory, \"\")) #random sampling test set (which is the test set utilized for AL performance)\n",
    "init_rs_df = pd.read_pickle(os.path.join(base_directory, \"\")) #initial seed set, that initializes the training dataset for both RS and MS heuristics\n",
    "\n",
    "#process stored datasets\n",
    "\n",
    "X_rs = clean_rs_df.drop(columns=[\"Class\", \"x\", \"y\", \"polygonID\", \"geotiff\", \"Pred certainty\"])\n",
    "y_rs = transform_y_2classes(clean_rs_df[\"Class\"])\n",
    "y7c_rs = clean_rs_df[\"Class\"]\n",
    "groups_rs = clean_rs_df[\"polygonID\"]\n",
    "\n",
    "init_X_rs = init_rs_df.drop(columns=[\"Class\", \"x\", \"y\", \"polygonID\", \"geotiff\", \"Pred certainty\"])\n",
    "init_y_rs = transform_y_2classes(init_rs_df[\"Class\"])\n",
    "init_y7c_rs = init_rs_df[\"Class\"]\n",
    "init_groups_rs = init_rs_df[\"polygonID\"]\n",
    "\n",
    "X_rs_train = rs_train.drop(columns=[\"Class\", \"x\", \"y\", \"polygonID\", \"geotiff\", \"Pred certainty\"])\n",
    "y_rs_train = transform_y_2classes(rs_train[\"Class\"])\n",
    "y7c_rs_train = rs_train[\"Class\"]\n",
    "groups_rs_train = rs_train[\"polygonID\"]\n",
    "\n",
    "X_rs_test = rs_test.drop(columns=[\"Class\", \"x\", \"y\", \"polygonID\", \"geotiff\", \"Pred certainty\"])\n",
    "y_rs_test = transform_y_2classes(rs_test[\"Class\"])\n",
    "y7c_rs_test = rs_test[\"Class\"]\n",
    "groups_rs_test = rs_test[\"polygonID\"]\n",
    "\n",
    "#normalize feature dataframes\n",
    "X_rs_norm = get_normalized_X(X_rs, norm)\n",
    "init_X_rs_norm = get_normalized_X(init_X_rs, norm)\n",
    "X_rs_train_norm = get_normalized_X(X_rs_train, norm)\n",
    "X_rs_test_norm = get_normalized_X(X_rs_test, norm)\n",
    "\n",
    "#rset dataset\n",
    "\n",
    "rset_df = pd.read_csv(os.path.join(base_directory, \"\"))\n",
    "rset_df.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "\n",
    "X_rset = rset_df.drop(columns=[\"C_ID_1\", \"x\", \"y\", \"groupID\"])\n",
    "y_rset = transform_y_2classes(rset_df[\"C_ID_1\"])\n",
    "y7c_rset = rset_df[\"C_ID_1\"]\n",
    "groups_rset = rset_df[\"groupID\"]\n",
    "\n",
    "X_rset_norm = get_normalized_X(X_rset, norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2e0a83-8b0e-4976-8faf-5ea4be6e477c",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "# Run BvSB, and store dataset to classify\n",
    "\n",
    "[Back to top](#Top)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14f1dcc-d02d-4cd0-afed-932d82ed7c46",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "# processDirectory.py\n",
    "\n",
    "* __Main part of processDirectory:__\n",
    "    * Read each geotiff iteratively and get the uncertainties for each point.\n",
    "    * Store the points with highest uncertainty.\n",
    "    * For that list, cut the points that were already discarded (labeled -1), or that appeared in a previous dataset.\n",
    "    * __Only after that, get the patches needed!__\n",
    "\n",
    "[Back to top.](#Top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b468af4-f618-4847-86c6-f4a368aef75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def geotiff_to_df(image_path, column_order=None, standardize=False, norm_dataframe=None):\n",
    "    \"\"\"\n",
    "    Converts a geotiff file into a pandas dataframe.\n",
    "    CORRECTED v3 - changed standardization to be global (using min max parameters from the entire region of interest) instead of batch-based normalization.\n",
    "    Assumes the images are on crs \"EPSG:4326\".\n",
    "    column_order: the order of bands in xds is different than the band order in model training. This parameter is to give the correct training column order.\n",
    "    \"\"\"\n",
    "    xds = rioxarray.open_rasterio(image_path, masked=True)\n",
    "    x_coords, y_coords = xds.coords['x'].values, xds.coords['y'].values\n",
    "\n",
    "    #for geotiffs with only one band, tipically the class band, simpler transformation, without normalization\n",
    "    if xds.shape[0] == 1:\n",
    "        values = xds[0].values.flatten()\n",
    "        df = pd.DataFrame({'x': list(x_coords)*len(y_coords),\n",
    "                           'y': y_coords.repeat(len(x_coords)),\n",
    "                           'band_0': values})\n",
    "        return df\n",
    "\n",
    "    names = xds.attrs[\"long_name\"]\n",
    "    df_final = pd.DataFrame(columns=xds.attrs[\"long_name\"])\n",
    "    \n",
    "    #correct column order if needed\n",
    "    if column_order is not None:\n",
    "        df_final = df_final[column_order]\n",
    "    #if standardize, flatten the 2D/3D array and apply min-max norm; else, just flatten the array\n",
    "    if standardize:\n",
    "        for i, name in enumerate(names):\n",
    "            values = xds[i].values.flatten() #each band has format (y_size, x_size); flatten() transform into 1d array with size (y_size * x_size)\n",
    "            norm_params = norm_dataframe.loc[norm[\"band\"]==name]\n",
    "            df_final[name] = (values - norm_params[\"min\"].iloc[0]) / (norm_params[\"max\"].iloc[0] - norm_params[\"min\"].iloc[0]) #iloc[0] due to FutureWarning; though pd.Series only has 1 element\n",
    "    else:\n",
    "        for i, name in enumerate(names):\n",
    "            values = xds[i].values.flatten()\n",
    "            df_final[name] = values\n",
    "\n",
    "    #merge lon and lat columns\n",
    "    if \"x\" not in df_final.columns:\n",
    "        df_final[\"x\"] = list(x_coords)*len(y_coords)\n",
    "        df_final[\"y\"] = y_coords.repeat(len(x_coords))\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "###############################################\n",
    "\n",
    "def get_margins(df, model, lon=None, lat=None, n=2):\n",
    "    \"\"\"\n",
    "    Receives: df, DataFrame with the training features\n",
    "    Returns: dataframe of uncertainty values for each example in df\n",
    "    \"\"\"\n",
    "    n_classes = len(model.classes_)\n",
    "    \n",
    "    margins = np.zeros(df.shape[0])\n",
    "    \n",
    "    #identify rows with nan values; for rows WITHOUT nans, compute margins; for rows WITH nans, leave it as nan\n",
    "    nan_mask = df.isna().any(axis=1)\n",
    "    margins[~nan_mask] = model.decision_function(df[~nan_mask])\n",
    "    margins[nan_mask] = np.nan\n",
    "\n",
    "    classes=np.sign(margins)\n",
    "    classes[classes==-1] = 2 #class 1 is cashew, class -1 is non-cashew, change to class 2\n",
    "    classes[classes==0] = 2 #where margin is 0, so we could say either class 1 or 2, it just to avoid errors that we specify one of them\n",
    "    \n",
    "    result_data = {\"1st class\": classes, \"Margin\": margins}\n",
    "    df_results = pd.DataFrame(result_data, dtype=\"float\")\n",
    "    #add longitude and latitude columns if provided\n",
    "    if lon is not None:\n",
    "        df_results[\"x\"] = lon\n",
    "    if lat is not None:\n",
    "        df_results[\"y\"] = lat\n",
    "\n",
    "    return df_results\n",
    "\n",
    "###############################################\n",
    "\n",
    "def process_file_margins(file_path, model, column_order=None, standardize=False, norm_dataframe=None):\n",
    "    \"\"\"\n",
    "    file_path to the GeoTIFF tile.\n",
    "    Transforms GeoTIFF into sklearn readable pandas df.\n",
    "    Determines class estimates and margins given a margin-based model\n",
    "    Returns: df, which is just the .tif transformed to pandas dataframe; uncertain dataframe with class and margin estimates \n",
    "    \"\"\"\n",
    "    df = geotiff_to_df(file_path, column_order=column_order, standardize=standardize, norm_dataframe=norm_dataframe)\n",
    "\n",
    "    longitude = df[\"x\"]\n",
    "    latitude = df[\"y\"]\n",
    "    df_nocoords = df.drop(columns=[\"x\", \"y\"])\n",
    "    uncertain = get_margins(df_nocoords, model, longitude, latitude)\n",
    "\n",
    "    return uncertain, df\n",
    "\n",
    "###############################################\n",
    "\n",
    "\n",
    "def process_directory_margins(model, directory_tif_coefficients=None, column_order=None, n_points=500,\n",
    "                         standardize=False, norm_dataframe=None, save_tif=False, directory_tif_predictions=None, random_seed=0):\n",
    "    \"\"\"\n",
    "    CHANGES: To deal with a directory where there are multiple folders, each indicating a subregion, which is partitioned into multiple GeoTIFFs\n",
    "    ...\n",
    "    directory_tif_coefficients: path to folder with the GeoTIFF coefficient files\n",
    "    model: model to determine class probability estimates\n",
    "    column_order: since order of features might change, depending on the training of each model\n",
    "    n_points or max_difference: 2 possible criteria to limit how many uncertain points the function returns\n",
    "    standardize: boolean, if we standardize data right after reading the GeoTIFF file\n",
    "    norm_dataframe: pandas dataframe with the means and std's of each training feature, for GLOBAL normalization\n",
    "    save_tif: save predictions tif (1 band image)\n",
    "    directory_tif_path: where to save the predictions tiff\n",
    "    \"\"\"\n",
    "    final_df_uncertain = pd.DataFrame() #store the difference, probable classes\n",
    "    final_df_coefs = pd.DataFrame() #store the training coefficients of the most uncertain points\n",
    "\n",
    "    #stipulate directories if needed\n",
    "    base_directory = os.getcwd()\n",
    "    if directory_tif_coefficients is None:\n",
    "        directory_tif_coefficients = os.path.join(base_directory, \"\")\n",
    "    if directory_tif_predictions is None:\n",
    "        training_set_folder = os.path.join(base_directory, \"\")\n",
    "        lastTrainingSet, lastTrainingNum = find_latest_file(directory = training_set_folder)\n",
    "        directory_tif_predictions = os.path.join(base_directory, \"\", str(f\"Iteration {lastTrainingNum}\"))\n",
    "        if not os.path.exists(directory_tif_predictions):\n",
    "            os.makedirs(directory_tif_predictions)\n",
    "    min_difference=1\n",
    "    #walk through the folders and files in the directory_tif_coefficients\n",
    "    for root, dirs, files in os.walk(directory_tif_coefficients):\n",
    "        for i, file_name in enumerate(files):\n",
    "            file = os.path.join(root,file_name)\n",
    "            print(\"Currently analyzing file: \", file)\n",
    "            \n",
    "            uncertain, coefs = process_file_margins(file, model, column_order, standardize, norm_dataframe)\n",
    "            uncertain[\"geotiff\"] = extract_number_from_filename(file_name) #to identify from which .tiff these results are\n",
    "                \n",
    "            #if we give max_difference as parameter, it will be the default rule; otherwise, restrict final_df by number of points \n",
    "            final_df_uncertain = pd.concat([final_df_uncertain, uncertain], axis=0, ignore_index=True)\n",
    "            final_df_coefs = pd.concat([final_df_coefs, coefs], axis=0, ignore_index=True)\n",
    "\n",
    "            min_margin = np.min(np.abs(final_df_uncertain[\"Margin\"]))\n",
    "            final_df_uncertain[\"absMargin\"] = -np.abs(final_df_uncertain[\"Margin\"]) #minus sign to select the smallest margins\n",
    "            final_df_uncertain = final_df_uncertain.nlargest(n_points,\"absMargin\", keep=\"all\")\n",
    "            final_df_uncertain.drop(columns=[\"absMargin\"], inplace=True)\n",
    "            final_df_coefs = final_df_coefs.loc[final_df_uncertain.index]\n",
    "            \n",
    "            #reset indexes\n",
    "            final_df_uncertain.reset_index(drop=True,inplace=True)\n",
    "            final_df_coefs.reset_index(drop=True,inplace=True)\n",
    "    \n",
    "            #save GeoTIFF with the predictions for all the pixels in the current tile\n",
    "            if save_tif:\n",
    "                save_tif_folder = directory_tif_predictions\n",
    "                \n",
    "                #create tif folder if it does not exist\n",
    "                if not os.path.exists(save_tif_folder):\n",
    "                    os.makedirs(save_tif_folder)\n",
    "                \n",
    "                with rio.open(file) as src:\n",
    "                    ras_meta = src.profile\n",
    "                    ras_meta['dtype'] = \"float32\"\n",
    "                    ras_meta[\"count\"] = 1\n",
    "                    \n",
    "                    #read the original data to get the mask\n",
    "                    original_data = src.read(1)\n",
    "                    mask = np.isnan(original_data)\n",
    "                \n",
    "                #reshape predictions to raster\n",
    "                raster_array = uncertain[\"1st class\"].to_numpy(dtype=\"float32\")\n",
    "                raster_array = raster_array.reshape((ras_meta[\"height\"], ras_meta[\"width\"]))                \n",
    "                raster_array[mask] = ras_meta['nodata']\n",
    "                #save as GeoTIFF\n",
    "                save_tif_path = os.path.join(save_tif_folder, file_name)\n",
    "                with rio.open(save_tif_path, 'w', **ras_meta) as dst:\n",
    "                    dst.write(raster_array, 1)\n",
    "                print(\"Saving file in: \", save_tif_path)\n",
    "\n",
    "        print(\"\\n #################################################### \\n\")\n",
    "    if (final_df_uncertain.loc[np.abs(final_df_uncertain[\"Margin\"]) <= min_margin]).shape[0] > n_points:\n",
    "        #in case there are still more than n_points, we do a final pick of which should be selected\n",
    "        sampled_indices = final_df_uncertain.sample(n=n_points, random_state=random_seed).index\n",
    "        final_df_uncertain = final_df_uncertain.loc[sampled_indices]\n",
    "        final_df_coefs = final_df_coefs.loc[sampled_indices]\n",
    "        final_df_uncertain.reset_index(drop=True,inplace=True)\n",
    "        final_df_coefs.reset_index(drop=True,inplace=True)\n",
    "        \n",
    "    return final_df_uncertain, final_df_coefs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b948b5-80bb-4966-bcbf-99a4fc4b77a2",
   "metadata": {},
   "source": [
    "______________\n",
    "\n",
    "# getPatches.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9793fd0-f060-4512-a967-0c4604236874",
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_inside_multi_polygon(x, y, multi_polygon):\n",
    "    point = Point(x, y)\n",
    "    return multi_polygon.contains(point)\n",
    "\n",
    "###############################################\n",
    "\n",
    "def obtain_patch_coordinates(tif_to_df_unique_x, tif_to_df_unique_y, point_coordinates, polygonID, verbose):\n",
    "    \"\"\"\n",
    "    Given coordinates for a point (central pixel), returns a dataframe with the coordinates for the direct 8 neighboring pixels. Works for a list of point_coordinates.\n",
    "    tif_to_df_unique_x and y regard the possible raster x and y coordinates.\n",
    "    polygonID is given to keep track of the unique patches. Every 3x3 pixel patch has a unique polygonID for identification and data split purposes.\n",
    "    \"\"\"\n",
    "    final_x = []\n",
    "    final_y = []\n",
    "    final_polygonID = []\n",
    "    for point in zip(point_coordinates[\"x\"], point_coordinates[\"y\"]):\n",
    "        x_indices = []\n",
    "        y_indices = []\n",
    "        central_index_x = np.where(tif_to_df_unique_x == point[0])[0][0] #double [0] to obtain the index\n",
    "        central_index_y = np.where(tif_to_df_unique_y == point[1])[0][0]\n",
    "        x_indices.append(tif_to_df_unique_x[central_index_x])\n",
    "        y_indices.append(tif_to_df_unique_y[central_index_y])\n",
    "        #evaluates if the central pixel is on any border of the raster. Special cases if it is, otherwise adds the 8 neighboring pixels.\n",
    "        if central_index_x == 0:\n",
    "            if verbose:\n",
    "                print(\"entered 1\")\n",
    "            x_indices.append(tif_to_df_unique_x[central_index_x+1])\n",
    "        elif central_index_x == (tif_to_df_unique_x.shape[0]-1):\n",
    "            if verbose: \n",
    "                print(\"entered 2\")\n",
    "            x_indices.append(tif_to_df_unique_x[central_index_x-1])\n",
    "        else:\n",
    "            x_indices.append(tif_to_df_unique_x[central_index_x+1])\n",
    "            x_indices.append(tif_to_df_unique_x[central_index_x-1])\n",
    "        if central_index_y == 0:\n",
    "            if verbose:\n",
    "                print(\"entered 4\")\n",
    "            y_indices.append(tif_to_df_unique_y[central_index_y+1])\n",
    "        elif central_index_y == (tif_to_df_unique_y.shape[0]-1):\n",
    "            if verbose:\n",
    "                print(\"entered 5\")\n",
    "            y_indices.append(tif_to_df_unique_y[central_index_y-1])\n",
    "        else:\n",
    "            y_indices.append(tif_to_df_unique_y[central_index_y+1])\n",
    "            y_indices.append(tif_to_df_unique_y[central_index_y-1]) \n",
    "        xy_coords = np.array(np.meshgrid(x_indices, y_indices)).T.reshape(-1,2) #from https://stackoverflow.com/questions/1208118/using-numpy-to-build-an-array-of-all-combinations-of-two-arrays\n",
    "        final_x.append(xy_coords[:,0])\n",
    "        final_y.append(xy_coords[:,1])\n",
    "        final_polygonID.append([polygonID for i in range(xy_coords.shape[0])])\n",
    "        polygonID += 1\n",
    "\n",
    "    df_return = pd.DataFrame({\"x\": np.concatenate(final_x), \"y\": np.concatenate(final_y), \"polygonID\": np.concatenate(final_polygonID)})\n",
    "    return df_return, polygonID #polygonID is also returned to keep track of the change in unique patches\n",
    "\n",
    "#########################################\n",
    "\n",
    "def get_patches_from_centroids(geotiffs_path, initial_df, num_indexes_to_return=None, check_inside_multipolygon=True, multi_polygon=None,\n",
    "                               column_order=None, standardize=None, norm_dataframe=None, random_seed=0, polygonID=5000, verbose=False):\n",
    "    \"\"\"\n",
    "    Adaptation from random_coefs_from_geotiffs()\n",
    "    Given list of points (centroids) from process_directory() for example, or any other font, obtain patch points around it, that is, the 8 neighboring pixels.\n",
    "    directory_path (str): The path to the directory containing the GeoTIFF files.\n",
    "    num_points_per_geotiff (int): The number of points to randomly select from each GeoTIFF.\n",
    "    num_indexes_to_return (int): The number of randomly selected indexes to return.\n",
    "    \"\"\"\n",
    "    final_df = pd.DataFrame()\n",
    "    random.seed(random_seed)\n",
    "    for root, dirs, files in os.walk(geotiffs_path):\n",
    "        for file in files:\n",
    "            file_num = extract_number_from_filename(file)\n",
    "            #check if file is a GeoTIFF\n",
    "            if (file_num in np.unique(initial_df[\"geotiff\"])) and (file.endswith(\".tif\") or file.endswith(\".tiff\")):\n",
    "                file_path = os.path.join(root, file)\n",
    "                print(f\"Processing GeoTIFF: {file}\")\n",
    "                #transform geotiff in dataframe\n",
    "                tif_to_df = geotiff_to_df(file_path)\n",
    "                #see which initial_df points are in this geotiff\n",
    "                in_geo_df = initial_df.loc[initial_df[\"geotiff\"]==file_num]\n",
    "                #obtain 3x3 patch dataset\n",
    "                xypatches, polygonID = obtain_patch_coordinates(np.unique(tif_to_df[\"x\"]), np.unique(tif_to_df[\"y\"]), in_geo_df, polygonID, verbose)\n",
    "                tif_to_df_patches = pd.merge(tif_to_df, xypatches[[\"x\",\"y\",\"polygonID\"]], on=[\"x\",\"y\"], how=\"inner\")\n",
    "                tif_to_df_patches[\"geotiff\"] = file_num\n",
    "                \n",
    "                if final_df.empty:\n",
    "                    final_df = tif_to_df_patches\n",
    "                else:\n",
    "                    final_df = pd.concat([final_df, tif_to_df_patches], ignore_index=True)\n",
    "                \n",
    "                if verbose:\n",
    "                    print(\"OG tif file size: \", tif_to_df.shape)\n",
    "                    print(\"in_geo_df size: \", in_geo_df.shape)\n",
    "                    print(\"Size with patches: \", tif_to_df_patches.shape)\n",
    "                    print(\"polygonID: \", polygonID)\n",
    "\n",
    "    #check if they are all inside the multipolygon\n",
    "    if (check_inside_multipolygon) and (multi_polygon is not None):\n",
    "        final_df.reset_index(drop=True,inplace=True)\n",
    "        if verbose:\n",
    "            print(\"Before inside_multi_polygon: \", final_df.shape)\n",
    "        inside_multi_polygon = [point_inside_multi_polygon(x, y, multi_polygon) for x, y in zip(final_df[\"x\"], final_df[\"y\"])]\n",
    "        final_df = final_df.loc[inside_multi_polygon]\n",
    "        if verbose:\n",
    "            print(\"After inside_multi_polygon: \", final_df.shape)\n",
    "    \n",
    "    #final cut of points from final_df\n",
    "    final_df.reset_index(drop=True,inplace=True)\n",
    "    if num_indexes_to_return is not None:\n",
    "        if verbose:\n",
    "            print(\"num_indexes_to_return is not None, cutting until desired number\")\n",
    "        selected_indexes = random.sample(range(len(final_df)), num_indexes_to_return)\n",
    "        final_df = final_df.iloc[selected_indexes]\n",
    "        final_df.reset_index(drop=True,inplace=True)\n",
    "\n",
    "    if standardize:\n",
    "        final_df[list(norm_dataframe[\"band\"])] = get_normalized_X(final_df[list(norm_dataframe[\"band\"])], norm_dataframe)\n",
    "    \n",
    "    #ORDER ACCORDING TO COLUMNS - polygonID, then x, then y\n",
    "    final_df = final_df.sort_values(by=['polygonID', 'x', 'y'], ignore_index=True)\n",
    "    \n",
    "    print(\"FINAL polygonID: \", polygonID)\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c241f9-5918-4291-9a3a-ed3ed74cfd5f",
   "metadata": {},
   "source": [
    "_____\n",
    "\n",
    "# savePickle.py\n",
    "\n",
    "[Back to top.](#Top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942548ad-0712-43e8-9458-285404af7824",
   "metadata": {},
   "outputs": [],
   "source": [
    "def savePickle(new_uncertain, new_coefs, max_num_points=None, save=False, training_set_folder=None, discarded_file_dir=None,\n",
    "               save_uncertain_folder=None, save_coefs_folder=None):\n",
    "    \"\"\"\n",
    "    Updated - the stored pickle file has the unlabeled points, that do not appear in the previously existing training dataset, instead of just previous pickle uncertain file\n",
    "    Also, in each iteration, the saved pickle only contains the newly highest uncertain points, not all the uncertain selected up to this iteration.\n",
    "    \"\"\"\n",
    "\n",
    "    #stipulate directories if needed\n",
    "    base_directory = os.getcwd()\n",
    "    if training_set_folder is None:\n",
    "        training_set_folder = os.path.join(base_directory, \"\")\n",
    "    lastTrainingSet, lastTrainingNum = find_latest_file(directory = training_set_folder)\n",
    "    file_to_save = str(f\"train_dataset_{lastTrainingNum}\")\n",
    "    #file that stores the discarded samples (those that were already selected in a previous AL iteration but could not be annotated)\n",
    "    if discarded_file_dir is None:\n",
    "        discarded_file_dir = os.path.join(base_directory, \"\")\n",
    "    #where I want to save\n",
    "    if save_uncertain_folder is None:\n",
    "        save_uncertain_folder = os.path.join(base_directory, \"\")\n",
    "    if save_coefs_folder is None:\n",
    "        save_coefs_folder = os.path.join(base_directory, \"\")\n",
    "        \n",
    "    directory_uncertain = os.path.join(save_uncertain_folder, file_to_save)\n",
    "    directory_coefs = os.path.join(save_coefs_folder, file_to_save)\n",
    "\n",
    "    #the files I want to compare to - most recently stored training set, and discarded file \n",
    "    try:\n",
    "        storedTrainingSet = pd.read_pickle(lastTrainingSet)\n",
    "    except FileNotFoundError:\n",
    "        storedTrainingSet = pd.DataFrame()\n",
    "    try:\n",
    "        discarded_file = pd.read_pickle(discarded_file_dir)\n",
    "    except FileNotFoundError:\n",
    "        discarded_file = pd.DataFrame()\n",
    "\n",
    "    print(lastTrainingSet)\n",
    "    print(discarded_file_dir)\n",
    "        \n",
    "    #get new points of uncertain and coefs dataframes\n",
    "    new_uncertain.reset_index(drop=True, inplace=True)\n",
    "    new_coefs.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    #no overlap with previous training datasets\n",
    "    newPointsIndex = filterOverlappingPoints(new_uncertain, storedTrainingSet)\n",
    "    print(newPointsIndex)\n",
    "    new_uncertain = new_uncertain.loc[newPointsIndex]\n",
    "    new_coefs = new_coefs.loc[newPointsIndex]    \n",
    "    new_uncertain.reset_index(drop=True, inplace=True) #reset again otherwise indices might work weird with filterOverlappingPoints\n",
    "    new_coefs.reset_index(drop=True, inplace=True)\n",
    "    #no points that were already discarded\n",
    "    nodiscPointsIndex = filterOverlappingPoints(new_uncertain, discarded_file) \n",
    "    print(nodiscPointsIndex)\n",
    "    new_uncertain = new_uncertain.loc[nodiscPointsIndex]\n",
    "    new_coefs = new_coefs.loc[nodiscPointsIndex]\n",
    "    new_uncertain.reset_index(drop=True, inplace=True) #reset again otherwise indices might work weird with filterOverlappingPoints\n",
    "    new_coefs.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    #perform a final cut/selection of points if needed\n",
    "    if (max_num_points is not None) and (new_uncertain.shape[0] > max_num_points):\n",
    "        index = find_n_lowest_difference_index(new_uncertain,max_num_points)\n",
    "        new_uncertain = new_uncertain.loc[index]\n",
    "        new_coefs = new_coefs.loc[index]\n",
    "\n",
    "    new_uncertain.reset_index(drop=True, inplace=True)\n",
    "    new_coefs.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    #save updated dataframes to specified or original pickle file\n",
    "    if save:\n",
    "        new_coefs.to_pickle(directory_coefs + \".pkl\")\n",
    "        new_uncertain.to_csv(directory_uncertain + \".csv\")\n",
    "\n",
    "    return new_uncertain, new_coefs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89406ba5-7342-4461-93df-4cfbe92705bc",
   "metadata": {},
   "source": [
    "________\n",
    "\n",
    "# BvSVB get and store dataset - runnable\n",
    "\n",
    "[Back to top](#Top)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50c32ab-2cf6-4bc8-89ad-68f7f43d47ab",
   "metadata": {},
   "source": [
    "* __Run BvSB__\n",
    "\n",
    "Atenção: não estou a usar corte de pontos/polígonos em mais nenhum lado, a única seleção de nº de pontos é em process_directory. Então, nº pontos aqi selecionado será o upper bound do nº de patches que vou ter de classificar (alguns podem ser removidos ou assim)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e389d22a-4e40-47cb-bb53-67ee8071cab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_directory = os.getcwd()\n",
    "clf_folder = os.path.join(base_directory, \"\") #call a specific model to get the column order. The model is not important, but consistent column order between AL iterations is\n",
    "latest_classifier, latest_classifier_num = find_latest_file(clf_folder) #latest classifier - trained with the latest, most upgraded training set\n",
    "\n",
    "print(\"Latest classifier:\")\n",
    "print(latest_classifier)\n",
    "\n",
    "clf = load(latest_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1122b8c0-e902-4de6-ad08-20e3c940af04",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "geojson_file = \"\"\n",
    "gdf = gpd.read_file(geojson_file)\n",
    "multi_polygon = gdf.unary_union\n",
    "\n",
    "uncertain, coefs = process_directory_margins(model=clf, column_order=clf.feature_names_in_, n_points=35, save_tif=True, standardize=True, norm_dataframe=norm,\n",
    "                                     directory_tif_predictions=None)# max_difference=1e-5,# standardize=True, norm_dataframe=norm,\n",
    "coefs[\"geotiff\"] = uncertain[\"geotiff\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64efcdd2-3875-445d-96a5-b809a511ba3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncertain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacbf63f-4fce-45c2-a209-4685fce36a95",
   "metadata": {},
   "source": [
    "* __Get patches__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff46b83-fb14-438b-9594-9702b36d657e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "geotiffs_path = os.path.join(base_directory, \"\")\n",
    "\n",
    "num_indexes_to_return = None\n",
    "patch_coefs = get_patches_from_centroids(geotiffs_path, coefs, random_seed=0, polygonID=5245, check_inside_multipolygon=False, standardize=True, norm_dataframe=norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10261d1d-62a8-48a5-b660-f9bae8d73e66",
   "metadata": {},
   "source": [
    "* __Cut patches (if needed) and get uncertainties__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1094f29-c6df-4f46-810c-90771931080c",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_uncertain = get_margins(patch_coefs.drop(columns=[\"x\",\"y\", \"polygonID\", \"geotiff\"]), clf, patch_coefs[\"x\"], patch_coefs[\"y\"])\n",
    "patch_uncertain[\"geotiff\"] = patch_coefs[\"geotiff\"]\n",
    "patch_uncertain[\"polygonID\"] = patch_coefs[\"polygonID\"]\n",
    "patch_uncertain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccddca8-b6bf-450e-8546-6689b41049db",
   "metadata": {},
   "source": [
    "* __Store points__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04736049-0c89-4f7f-9844-96951ee581df",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_uncertain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433dccc0-4496-4173-a336-28afbd94262a",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242e3be9-e32b-440d-840e-2f22cc4090d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_uncertain2, patch_coefs2 = savePickle(patch_uncertain, patch_coefs, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b1b636-b999-4c29-af9c-16f868c6cf2a",
   "metadata": {},
   "source": [
    "__________\n",
    "\n",
    "# Update datasets and trained model\n",
    "\n",
    "[Back to top](#Top)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f454dfc4-4446-4387-a4de-a5a34245951a",
   "metadata": {},
   "source": [
    "_____\n",
    "\n",
    "# updatePickleAndTrainingSet.py\n",
    "\n",
    "[Back to top.](#Top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0a6743-7d90-4dbd-9268-06b928ec803f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def concatenateClassToPickle(pickle_file, csv_file):\n",
    "    \"\"\"\n",
    "    Classes can be written on Uncertain .csv file. But in python we are always working with the pickle.\n",
    "    So, get Class from .csv to the .pkl file.\n",
    "    Should be used for both the Data/Coefs and Data/Uncertain files\n",
    "    Note: csv file can have row order changed, as long as we have \"Unnamed: 0\" column to re-order it\n",
    "    \"\"\"\n",
    "    pickle = pd.read_pickle(pickle_file)\n",
    "    csv = pd.read_csv(csv_file,encoding=\"latin-1\") #because of ? characters\n",
    "    #order columns first\n",
    "    csv.set_index(csv[\"Unnamed: 0\"],inplace=True)\n",
    "    csv.sort_index(inplace=True) \n",
    "\n",
    "    pickle[\"Class\"] = csv[\"Class\"]\n",
    "    pickle[\"Pred certainty\"] = csv[\"Pred certainty\"]\n",
    "    pickle.to_pickle(pickle_file)\n",
    "    return;\n",
    "\n",
    "###############################################\n",
    "\n",
    "def find_latest_pickle(directory):\n",
    "    \"\"\"\n",
    "    Given directory, gets latest pickle file (with complete directory), and the number that appears right before the .extension (someName_\"number\".extension)\n",
    "    Only implemented for pickle files, because!\n",
    "    \"\"\"\n",
    "    pickle_files = [file for file in os.listdir(directory) if file.endswith(\".pkl\")]\n",
    "    if not pickle_files:\n",
    "        return None, -1 \n",
    "\n",
    "    #extract suffix numbers from file names and find the maximum\n",
    "    suffix_numbers = [int(file.split(\"_\")[-1].split(\".\")[0]) for file in pickle_files]\n",
    "    latest_suffix = max(suffix_numbers)\n",
    "    latest_pickle_file = str(f\"train_dataset_{latest_suffix}.pkl\")\n",
    "\n",
    "    return latest_pickle_file, latest_suffix\n",
    "\n",
    "###############################################\n",
    "\n",
    "def filter_by_column_value(df, column_name=\"Class\", value=-1):\n",
    "    df_not_value = df.loc[df[column_name] != value]\n",
    "    df_value = df.loc[df[column_name] == value][[\"x\",\"y\"]] # we only need their positions\n",
    "    df_not_value.reset_index(drop=True, inplace=True)\n",
    "    df_value.reset_index(drop=True, inplace=True)\n",
    "    return df_not_value, df_value\n",
    "\n",
    "###############################################\n",
    "\n",
    "def concatenateTrainingSet(train_set_directory=None, train_features_directory=None, discard_file=None, save=False):\n",
    "    \"\"\"\n",
    "    Checks Data/Coefs, with newly added labels.\n",
    "    Checks last saved training dataset.\n",
    "    Creates a new training dataset, merging the 2.\n",
    "    \"\"\"\n",
    "\n",
    "    #stipulate directories if needed\n",
    "    base_directory = os.getcwd()\n",
    "    if train_set_directory is None:\n",
    "        train_set_directory = os.path.join(base_directory, \"\")\n",
    "    if train_features_directory is None:\n",
    "        train_features_directory = os.path.join(base_directory, \"\")\n",
    "\n",
    "    latest_pickle_file, latest_suffix = find_latest_pickle(train_features_directory)\n",
    "    latest_suffix += 1\n",
    "\n",
    "    #from the last saves pixel, which contains a column with the indication of the class, filter which samples are kept and added to the train set, and those that are discarded\n",
    "    #SAMPLES TO DISCARD HAD THEIR CLASS ASSIGNED AS -1 DURING ANNOTATION\n",
    "    df1 = pd.read_pickle(os.path.join(train_features_directory, latest_pickle_file))\n",
    "    print(os.path.join(train_features_directory, latest_pickle_file))\n",
    "    print(os.path.join(train_set_directory, latest_pickle_file))\n",
    "    df1_keep, df1_discard = filter_by_column_value(df1)\n",
    "\n",
    "    if latest_pickle_file is None:\n",
    "        result_df = df1_keep\n",
    "    else:\n",
    "        df2 = pd.read_pickle(os.path.join(train_set_directory, latest_pickle_file))\n",
    "        result_df = pd.concat([df2, df1_keep], axis=0, ignore_index=True)\n",
    "\n",
    "    #store coordinates of the discarded points\n",
    "    if discard_file is not None:\n",
    "        try:\n",
    "            discard_df = pd.read_pickle(discard_file)\n",
    "            df1_discard_index = filterOverlappingPoints(df1_discard, discard_df)\n",
    "            print(discard_df.index)\n",
    "            print(df1_discard.index)\n",
    "            print(df1_discard_index)\n",
    "            df1_discard = df1_discard.loc[df1_discard_index]\n",
    "            discard_df = pd.concat([discard_df,df1_discard], axis=0, ignore_index=True)\n",
    "        except FileNotFoundError:\n",
    "            discard_df = df1_discard\n",
    "        discard_df.reset_index(drop=True, inplace=True)\n",
    "        discard_df.to_pickle(discard_file)\n",
    "    \n",
    "    if save:\n",
    "        new_pickle_file = os.path.join(train_set_directory, f\"train_dataset_{latest_suffix}.pkl\")\n",
    "        result_df.to_pickle(new_pickle_file)\n",
    "    return result_df\n",
    "\n",
    "###############################################\n",
    "\n",
    "def updateSets(training_directory=None, uncertain_directory=None, coefs_directory=None, discard_file=None, save=False):\n",
    "    base_directory = os.getcwd()\n",
    "    if training_directory == None:\n",
    "        training_directory = os.path.join(base_directory, \"\")\n",
    "    if uncertain_directory == None:\n",
    "        uncertain_directory = os.path.join(base_directory, \"\")\n",
    "    if coefs_directory == None:\n",
    "        coefs_directory = os.path.join(base_directory, \"\")\n",
    "\n",
    "    last_run, last_run_num = find_latest_pickle(training_directory)\n",
    "\n",
    "    coefs_pickle_file = os.path.join(coefs_directory, f\"train_dataset_{last_run_num}.pkl\")\n",
    "    uncertain_csv_file = os.path.join(uncertain_directory, f\"train_dataset_{last_run_num}.csv\")\n",
    "\n",
    "    #pass the classes on the annotated csv file to the training features file \n",
    "    concatenateClassToPickle(coefs_pickle_file,uncertain_csv_file)\n",
    "\n",
    "    #stipulate the discard directory/file\n",
    "    if discard_file is None:\n",
    "        discard_file = os.path.join(base_directory, \"Discarded.pkl\")\n",
    "\n",
    "    print(coefs_pickle_file)\n",
    "    print(uncertain_csv_file)\n",
    "    #update training set with kept samples\n",
    "    result_df = concatenateTrainingSet(training_directory, discard_file=discard_file, save=save)\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb09edd8-b43f-42a0-9409-0ed106055814",
   "metadata": {},
   "source": [
    "______________\n",
    "\n",
    "# Update datasets and trained model - runnable\n",
    "\n",
    "[Back to top](#Top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e0e6d5-ddea-4335-9c5e-a3faed211f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "base_directory = os.getcwd()\n",
    "training_directory = os.path.join(base_directory, \"\")\n",
    "uncertain_directory = os.path.join(base_directory, \"\")\n",
    "coefs_directory = os.path.join(base_directory, \"\")\n",
    "discard_file = os.path.join(base_directory, \"\")\n",
    "\n",
    "#identify last AL iteration\n",
    "last_run, last_run_num = find_latest_pickle(directory = training_directory)\n",
    "\n",
    "result_df = updateSets(training_directory, uncertain_directory=None, coefs_directory=None, discard_file=None, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fa2e94-438d-44f4-b5ca-ec24d48a669d",
   "metadata": {},
   "source": [
    "* __Retrain model with updated training set and save__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c1606b-29ff-4b79-a812-ac24881c1a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_update = result_df.drop(columns=[\"Class\",\"x\",\"y\", \"polygonID\", \"geotiff\", \"Pred certainty\"])\n",
    "y_train_update = transform_y_2classes(result_df[\"Class\"])\n",
    "\n",
    "svm_params = {'C': 0.0117, 'class_weight': \"balanced\", 'degree': 4, 'gamma': 0.6439, 'kernel': \"poly\"} #best params after HP tuning\n",
    "clf = SVC(**svm_params, random_state=42)\n",
    "\n",
    "clf.fit(X_train_update,y_train_update)\n",
    "\n",
    "filename = os.path.join(base_directory, \"\", f\"svm_{last_run_num+1}\") \n",
    "dump(clf, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0624375a-3e8d-43e8-abae-b15d6a3bd277",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "# AL iter performance\n",
    "\n",
    "[Back to top](#Top)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b2dcb4-9d69-4bf6-950e-6e6ed17793c7",
   "metadata": {},
   "source": [
    "* __MS-AL results__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72b86ce-d45e-4dee-b37d-d947260f9ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_params = {'C': 0.0117, 'class_weight': \"balanced\", 'degree': 4, 'gamma': 0.6439, 'kernel': \"poly\"}\n",
    "clf = SVC(**svm_params, random_state=42)\n",
    "clf.fit(X_rs_train_norm, y_rs_train)\n",
    "\n",
    "base_directory = os.getcwd()\n",
    "models_directory = os.path.join(base_directory, \"\")\n",
    "training_sets_directory = os.path.join(base_directory, \"\")\n",
    "line1_values = [balanced_accuracy_scorer(clf, X_rs_test_norm, y_rs_test), f1_cashew_scorer(clf, X_rs_test_norm, y_rs_test)]\n",
    "\n",
    "n_iters = len(os.listdir(training_sets_directory))\n",
    "\n",
    "iters=np.arange(n_iters)\n",
    "train_sizes=[]\n",
    "f1_train=[]\n",
    "balacc_train=[]\n",
    "f1_test=[]\n",
    "balacc_test=[]\n",
    "ovacc_train=[]\n",
    "ovacc_test=[]\n",
    "\n",
    "models = os.listdir(models_directory)\n",
    "for i,train_file in enumerate(os.listdir(training_sets_directory)):\n",
    "    #get train sizes\n",
    "    train_path = os.path.join(training_sets_directory, train_file)\n",
    "    train_set = pd.read_pickle(train_path)\n",
    "    train_set = train_set.loc[train_set[\"Class\"]>0]\n",
    "    train_sizes.append(train_set.shape[0])\n",
    "    #get model results\n",
    "    model_path = os.path.join(models_directory, models[i])\n",
    "    clf = load(model_path) \n",
    "    f1_train.append(f1_cashew_scorer(clf, train_set[norm[\"band\"]], transform_y_2classes(train_set[\"Class\"])))\n",
    "    balacc_train.append(balanced_accuracy_scorer(clf, train_set[norm[\"band\"]], transform_y_2classes(train_set[\"Class\"])))\n",
    "    f1_test.append(f1_cashew_scorer(clf, X_rs_test_norm, y_rs_test))\n",
    "    balacc_test.append(balanced_accuracy_scorer(clf, X_rs_test_norm, y_rs_test))\n",
    "    ovacc_train.append(clf.score(train_set[norm[\"band\"]], transform_y_2classes(train_set[\"Class\"])))\n",
    "    ovacc_test.append(clf.score(X_rs_test_norm,y_rs_test))\n",
    "\n",
    "#fill results_ms dataframe\n",
    "results_ms = pd.DataFrame()\n",
    "results_ms[\"iterID\"] = iters\n",
    "results_ms[\"trainSize\"] = train_sizes\n",
    "results_ms[\"ovacc_train\"] = ovacc_train\n",
    "results_ms[\"balacc_train\"] = balacc_train\n",
    "results_ms[\"f1_train\"] = f1_train\n",
    "results_ms[\"ovacc_test\"] = ovacc_test\n",
    "results_ms[\"balacc_test\"] = balacc_test\n",
    "results_ms[\"f1_test\"] = f1_test\n",
    "\n",
    "results_ms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f53e8d-e2e4-47d7-891b-e9c3b97f3e62",
   "metadata": {},
   "source": [
    "* __RS results__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d6f8d0-9af7-4d2a-b63f-28c4361ceafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_params = {'C': 0.0117, 'class_weight': \"balanced\", 'degree': 4, 'gamma': 0.6439, 'kernel': \"poly\"}\n",
    "clf = SVC(**svm_params, random_state=42)\n",
    "clf.fit(X_rs_train_norm, y_rs_train)\n",
    "\n",
    "\n",
    "results_maxrs = pd.DataFrame()\n",
    "results_maxrs[\"iterID\"] = [min(np.min(results_rs[\"iterID\"]), np.min(results_ms[\"iterID\"])), max(np.max(results_rs[\"iterID\"]), np.max(results_ms[\"iterID\"]))]\n",
    "results_maxrs[\"trainSize\"] = [min(np.min(results_rs[\"trainSize\"]), np.min(results_ms[\"trainSize\"])), max(np.max(results_rs[\"trainSize\"]), np.max(results_ms[\"trainSize\"]))]\n",
    "results_maxrs[\"ovacc_train\"] = [clf.score(X_rs_train_norm, y_rs_train) for i in range(2)]\n",
    "results_maxrs[\"balacc_train\"] = [balanced_accuracy_scorer(clf, X_rs_train_norm, y_rs_train) for i in range(2)]\n",
    "results_maxrs[\"f1_train\"] = [f1_cashew_scorer(clf, X_rs_train_norm, y_rs_train) for i in range(2)]\n",
    "results_maxrs[\"ovacc_test\"] = [clf.score(X_rs_test_norm, y_rs_test) for i in range(2)]\n",
    "results_maxrs[\"balacc_test\"] = [balanced_accuracy_scorer(clf, X_rs_test_norm, y_rs_test) for i in range(2)]\n",
    "results_maxrs[\"f1_test\"] = [f1_cashew_scorer(clf, X_rs_test_norm, y_rs_test) for i in range(2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68af4722-86ef-4883-86e1-fd95b1edfa2e",
   "metadata": {},
   "source": [
    "* __Plots__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa7cca1-65c6-4ce8-80f8-43e1004debf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract iterative step\n",
    "plot_maxrs=True #if you want to plot an horizontal line with the performance of the full RS train set model\n",
    "results = [results_rs, results_ms, results_maxrs]\n",
    "columns_to_plot = ['ovacc_train', 'balacc_train', 'f1_train', 'ovacc_test', 'balacc_test', 'f1_test']\n",
    "titles_to_plot = [\"Ov. Accuracy train\", \"Bal. Accuracy train\", \"F1 train\", \"Ov. Accuracy test\", \"Bal. Accuracy test\", \"F1 test\"]\n",
    "\n",
    "plt.figure(figsize = (18,8))\n",
    "for i, col in enumerate(columns_to_plot):\n",
    "    plt.subplot(2,3,i+1) #6 plots, 2x3 matrix\n",
    "        \n",
    "    plt.plot(results[0][\"trainSize\"], results[0][col], marker='o', linestyle='-', label=\"RS\")\n",
    "    plt.plot(results[1][\"trainSize\"], results[1][col], marker='o', linestyle='-', label=\"MS-AL\")\n",
    "    if plot_maxrs:\n",
    "        plt.plot(results[2][\"trainSize\"], results[2][col], linestyle='--', label=\"Full RS train set\")\n",
    "    \n",
    "    plt.xlabel('trainSize')\n",
    "    plt.ylabel('Performance')\n",
    "    plt.title(titles_to_plot[i])\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a8188c-8822-44ff-bc30-b27a31898c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5412af-e4ea-449a-a8f7-a3e50d4e7f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_rs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cc7fed-2359-4280-aacf-8d69d536c4b9",
   "metadata": {},
   "source": [
    "________________\n",
    "\n",
    "# Stats per iteration\n",
    "\n",
    "[Back to top](#Top)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cafccb7-0108-4860-8c3f-355c9b3c2e97",
   "metadata": {},
   "source": [
    "* __Class proportions per AL iteration__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb643c2a-00ce-4834-8b27-049556d817a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "colors = {-1: \"black\", 1: '#006600', 2: '#99ff33', 3: \"#2d8659\", 4: '#c6538c', 5: '#808000', 6: '#804000', 7: '#0000ff', 8:'orange'}\n",
    "\n",
    "base_directory = os.getcwd()\n",
    "models_directory = os.path.join(base_directory, \"\")\n",
    "training_sets_directory = os.path.join(base_directory, \"\")\n",
    "coef_sets_directory = os.path.join(base_directory, \"\")\n",
    "n_iters = len(os.listdir(coef_sets_directory))\n",
    "\n",
    "proportions = []\n",
    "\n",
    "iters=np.arange(n_iters)\n",
    "for i,train_file in enumerate(os.listdir(coef_sets_directory)):\n",
    "    train_path = os.path.join(coef_sets_directory, train_file)\n",
    "    train_set = pd.read_pickle(train_path)\n",
    "\n",
    "    #we ended up with 8 MS-AL iterations in total, so divided the plots into a 4x2 subplot matrix \n",
    "    plt.subplot(len(iters)//4+1,4,i+1)\n",
    "\n",
    "    #barplots of pixel counts per class\n",
    "    class_counts = train_set['Class'].value_counts()\n",
    "    class_counts = class_counts.sort_index()\n",
    "    class_counts.plot(kind='bar', color=[colors[cls] for cls in class_counts.index])\n",
    "    \n",
    "    plt.title(f'Iteration {i+1}')\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "    proportion = class_counts.get(-1, 0) / len(train_set)\n",
    "    proportions.append([i+1, len(train_set), proportion.round(3)])\n",
    "     \n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "headers = [\"Iteration\", \"Train Set Size\", \"Proportion of Class -1\"]\n",
    "print(tabulate(proportions, headers=headers, tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619ec335-e4d1-4bbc-893b-8d1db9ea5077",
   "metadata": {},
   "source": [
    "* __Percentage of discarded points per AL iteration__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91aeb222-12c6-4a3d-b2bb-aca83a985ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "proportions = np.array(proportions)\n",
    "plt.grid()\n",
    "plt.gca().set_axisbelow(True)\n",
    "plt.axvline(5, linestyle=\"--\", color=\"orange\", label=\"Peak performance iteration\")\n",
    "plt.scatter(proportions[:,0]+1, proportions[:,2])\n",
    "plt.legend()\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"% of discarded points\")\n",
    "plt.title(\"Percentage of Discarded Points per Iteration\");\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"ms_percentage_discarded_points_per_iteration.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327c2d34-c4fd-4956-ac17-f80303ce8236",
   "metadata": {},
   "source": [
    "________________\n",
    "\n",
    "# Bottom\n",
    "\n",
    "[Back to Top.](#Top)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
