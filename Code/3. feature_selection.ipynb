{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bee52703-0634-4eb0-9100-7b7d2e05a4bf",
   "metadata": {},
   "source": [
    "______\n",
    "\n",
    "# Top\n",
    "\n",
    "* [Setup](#Setup)\n",
    "* [Download datasets](#Download-datasets)\n",
    "* [Feature selection](#Feature-selection)\n",
    "    * [Above average threshold cut](#Above-average-threshold-cut)\n",
    "    * [RFECV cut](#RFECV-cut)\n",
    "* [Hyperparameter tuning](#Hyperparameter-tuning)\n",
    "    * [HP for Random Forests](#HP-for-Random-Forests)\n",
    "    * [HP for SVMs](#HP-for-SVMs)\n",
    "\n",
    "[Bottom](#Bottom)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37343922-d0aa-4e5c-b95c-e6068027d71f",
   "metadata": {},
   "source": [
    "_____\n",
    "\n",
    "# Setup\n",
    "\n",
    "[Back to Top](#Top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2ce2fd-dafa-4368-ac9f-04518a075571",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "##/////////////////////////////////////\n",
    "## Imports and Util\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedGroupKFold, cross_val_score\n",
    "from sklearn.metrics import f1_score, accuracy_score, balanced_accuracy_score\n",
    "from sklearn.feature_selection import RFECV, SequentialFeatureSelector\n",
    "\n",
    "from hyperopt import hp, tpe, fmin\n",
    "\n",
    "import time\n",
    "import os\n",
    "\n",
    "base_directory=os.getcwd()\n",
    "class_names = ['Closed Forest', 'Open Forest', 'Mangrove', 'Savanna', 'Cashew', 'Non-Forest', 'Water']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb452ba-6a25-4747-a1b7-f2de511bb2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "##/////////////////////////////////////\n",
    "## Auxiliary functions\n",
    "\n",
    "def transform_y_2classes(y):\n",
    "    \"\"\"\n",
    "    Transforms a y labeled array/Series from the 7 classes, where 5 is cashew, into a 2 labeled Series where 1 cashew, 2 non-cashew\n",
    "    \"\"\"\n",
    "    y_update = pd.Series(y == 5, dtype=\"int\")\n",
    "    y_update.loc[y_update==0] = 2\n",
    "    return y_update\n",
    "\n",
    "##############################################\n",
    "\n",
    "def get_feature_importances(rf, X_train):\n",
    "    \"\"\"\n",
    "    Gets feature importances for a training dataset using a trained Random Forest Classifier or any other algorithm with a feature_importances_ attribute\n",
    "    \"\"\"\n",
    "    feature_importances = rf.feature_importances_\n",
    "    feature_names = X_train.columns\n",
    "    feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
    "    feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "    feature_importance_df.reset_index(inplace=True)\n",
    "    return feature_importance_df\n",
    "\n",
    "##############################################\n",
    "\n",
    "def categorize_feature(feature_name):\n",
    "    \"\"\"\n",
    "    Features in our project came from 3 different sources: direct band features, spatial features (GLCM) or temporal features (CCDC).\n",
    "    Determines the family of a feature based on its feature name.\n",
    "    \"\"\"\n",
    "    parts = feature_name.split('_')\n",
    "    if len(parts) == 1:\n",
    "        return 'band'\n",
    "    elif len(parts) == 2 and parts[1].isupper():\n",
    "        return 'temporal'\n",
    "    elif len(parts) == 2 and parts[1].islower():\n",
    "        return 'spatial'\n",
    "    else:\n",
    "        return 'unknown'\n",
    "\n",
    "##############################################\n",
    "\n",
    "def categorize_suffix(feature_name):\n",
    "    \"\"\"\n",
    "    For a feature name that contains a \"_\", splits the name based on that character\n",
    "    \"\"\"\n",
    "    if '_' in feature_name:\n",
    "        parts = feature_name.split('_')\n",
    "        return parts[1]\n",
    "    else:\n",
    "        return feature_name\n",
    "\n",
    "##############################################\n",
    "\n",
    "def plot_n_top_bottom(feature_importance_df, n=100, plot_band_temporal_spatial_count=True, plot_suffix_count=True):\n",
    "    \"\"\"\n",
    "    Creates a plot regarding the nature/family of the top and bottom features according to a feature score, stored in feature_importance_df.\n",
    "    \"\"\"\n",
    "    if feature_importance_df.shape[0] < n:\n",
    "        n = feature_importance_df.shape[0]\n",
    "    \n",
    "    #plot a barplot with the indication of the nature of features according: being spatial, temporal, or a direct Sentinel-2 band\n",
    "    if plot_band_temporal_spatial_count:\n",
    "        df = feature_importance_df.copy(deep=True)\n",
    "        df['Category'] = df['Feature'].apply(categorize_feature)\n",
    "        #sort df based on feature importance column\n",
    "        df_sorted = df.sort_values(by='Importance', ascending=False)\n",
    "        top_n = df_sorted.head(n)\n",
    "        bottom_n = df_sorted.tail(n)\n",
    "        \n",
    "        #plotting counts\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "        top_n['Category'].value_counts().plot(kind='bar', ax=axs[0], color='skyblue')\n",
    "        axs[0].set_title(f'Top {n} Important Features')\n",
    "        bottom_n['Category'].value_counts().plot(kind='bar', ax=axs[1], color='salmon')\n",
    "        axs[1].set_title(f'Bottom {n} Important Features')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    #plot a barplot with the direct suffixes of the features\n",
    "    if plot_suffix_count:\n",
    "        df = feature_importance_df.copy(deep=True)\n",
    "        df['Category'] = df['Feature'].apply(categorize_suffix)\n",
    "        #select just the top and bottom n\n",
    "        df_sorted = df.sort_values(by='Importance', ascending=False)\n",
    "        top_n = df_sorted.head(n)\n",
    "        bottom_n = df_sorted.tail(n)\n",
    "\n",
    "        #plots\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        \n",
    "        top_n['Category'].value_counts().plot(kind='bar', ax=axs[0], color='skyblue')\n",
    "        axs[0].set_title(f'Top {n} Important Features')\n",
    "        axs[0].set_xticklabels(axs[0].get_xticklabels(), rotation=45, ha='right')\n",
    "        \n",
    "        bottom_n['Category'].value_counts().plot(kind='bar', ax=axs[1], color='salmon')\n",
    "        axs[1].set_title(f'Bottom {n} Important Features')\n",
    "        axs[1].set_xticklabels(axs[1].get_xticklabels(), rotation=45, ha='right')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    return;\n",
    "\n",
    "##############################################\n",
    "\n",
    "def find_common_and_unique_strings(list1, list2):\n",
    "    \"\"\"\n",
    "    For two lists of strings, identify the common strings and those unique to each list.\n",
    "    \"\"\"\n",
    "    set1 = set(list1)\n",
    "    set2 = set(list2)\n",
    "    \n",
    "    #find common strings\n",
    "    common_strings = set1.intersection(set2)\n",
    "    \n",
    "    #find strings appearing only in list1 or list2\n",
    "    unique_to_list1 = set1.difference(set2)\n",
    "    unique_to_list2 = set2.difference(set1)\n",
    "    \n",
    "    return list(common_strings), list(unique_to_list1), list(unique_to_list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c338d30-d599-4988-ba93-947a89129847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balanced_accuracy_scorer(estimator, X_true, y_true):\n",
    "    y_pred = estimator.predict(X_true)\n",
    "    if (np.unique(y_pred) != np.unique(y_true)).all():\n",
    "        print(\"Classes in true labels and predictions mismatch!\")\n",
    "        print(\"Unique preds: \", np.unique(y_pred))\n",
    "        print(\"Unique trues\", np.unique(y_true))\n",
    "    return balanced_accuracy_score(y_true,y_pred)\n",
    "\n",
    "def f1_cashew_scorer(estimator, X_true, y_true):\n",
    "    \"\"\"\n",
    "    Return F1 score for cashew. Adapted to the 7-class system where cashew is class 5, or to the binary system where cashew is class 1\n",
    "    \"\"\"\n",
    "    y_pred = estimator.predict(X_true)\n",
    "    if (np.unique(y_pred) != np.unique(y_true)).all():\n",
    "        print(\"Classes in true labels and predictions mismatch!\")\n",
    "        print(\"Unique preds: \", np.unique(y_pred))\n",
    "        print(\"Unique trues\", np.unique(y_true))\n",
    "    if np.unique(y_true).shape[0] == 2:\n",
    "        return f1_score(y_true,y_pred,average=None)[0] #assumes 1st class is cashew, 2nd is non      \n",
    "    else:\n",
    "        try:\n",
    "            return f1_score(y_true,y_pred,average=None)[4] #in the 7 total classes, cashew is 5th\n",
    "        except IndexError: #other number of classes, could be just 1, if cashew doesn't appear for example\n",
    "            return -1\n",
    "\n",
    "########################################################################################################\n",
    "\n",
    "def plot_train_test_results(model, X_train, y_train, X_test, y_test, class_names):\n",
    "    \"\"\"\n",
    "    Plots the train and test results for a given model following a given set of classes\n",
    "    \"\"\"\n",
    "    \n",
    "    model.fit(X_train,y_train)\n",
    "    #different treatment for a cashew vs. non-cashew binary classification system, and for the eight-class system\n",
    "    if np.unique(y_train).shape[0] == 2:\n",
    "        labels_cm = np.array([0,1])\n",
    "        labels_recall = [1]\n",
    "    else:\n",
    "        labels_cm = np.arange(1,8)\n",
    "        labels_recall = [5]\n",
    "\n",
    "    #train set results\n",
    "    preds_train = model.predict(X_train)\n",
    "    print(\"Ov. Accuracy train: \", np.round(model.score(X_train,y_train),3))\n",
    "    print(\"Balanced Accuracy train: \", np.round(balanced_accuracy_scorer(model, X_train, y_train),3))\n",
    "    print(\"Recall cashew train: \", np.round( metrics.recall_score(y_train, preds_train, labels=labels_recall, average='micro'), 3) )\n",
    "    print(\"F1-score cashew train: \", np.round(f1_cashew_scorer(model, X_train, y_train),3))\n",
    "    print(\"Every F1-score train: \", np.round(f1_score(y_train,preds_train,average=None),3))\n",
    "    print(\"\\n\")\n",
    "    cm = metrics.confusion_matrix(y_train,preds_train,labels=labels_cm)\n",
    "    cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = cm,display_labels=np.array(class_names))\n",
    "    cm_display.plot()\n",
    "    plt.xticks(rotation = 45)\n",
    "    plt.show()\n",
    "    \n",
    "    #test set results\n",
    "    preds = model.predict(X_test)\n",
    "    print(\"Ov. Accuracy test: \", np.round(model.score(X_test,y_test),3))\n",
    "    print(\"Balanced Accuracy test: \", np.round(balanced_accuracy_scorer(model, X_test, y_test),3))\n",
    "    print(\"Recall cashew test: \", np.round( metrics.recall_score(y_test, preds, labels=labels_recall, average='micro'), 3) )\n",
    "    print(\"F1-score cashew test: \", np.round(f1_cashew_scorer(model, X_test, y_test),3))\n",
    "    print(\"Every F1-score test: \", np.round(f1_score(y_test,preds,average=None),3))\n",
    "    cm = metrics.confusion_matrix(y_test,preds,labels=labels_cm)\n",
    "    cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = cm,display_labels=np.array(class_names))\n",
    "    cm_display.plot()\n",
    "    plt.xticks(rotation = 45)\n",
    "    plt.show()\n",
    "    \n",
    "    return;\n",
    "\n",
    "########################################################################################################\n",
    "\n",
    "def hyperopt_get_best(X_train, y_train, scorer, max_evals=30, timeout=7200, space=None, just_ints=False):\n",
    "    \"\"\"\n",
    "    Bayesian hyperparameter optimization strategy.\n",
    "    Selects the best HP combination based on mean 5-fold cross validation score on the training dataset\n",
    "    \"\"\"\n",
    "    def objective(params):\n",
    "        clf = RandomForestClassifier(**params)\n",
    "        return -np.mean(cross_val_score(clf, X_train, y_train, cv=5, n_jobs=3, scoring=scorer))\n",
    "\n",
    "    #define feature space if not stipulated; the default is the space for the Random Forests\n",
    "    if space is None:\n",
    "        space = {\n",
    "            'n_estimators': hp.uniformint(\"n_estimators\", low=50, high=400, q=10),\n",
    "            'max_depth': hp.uniformint(\"max_depth\", low=1, high=15, q=3), \n",
    "            'max_features': hp.uniformint(\"max_features\", low=3, high=250, q=7)\n",
    "        }\n",
    "\n",
    "    #run the optimizer\n",
    "    best = fmin(fn=objective,\n",
    "                space=space,\n",
    "                algo=tpe.suggest,\n",
    "                rstate=np.random.RandomState(42),\n",
    "                max_evals=max_evals,\n",
    "                timeout=timeout\n",
    "               )\n",
    "\n",
    "    #print the best hyperparameters\n",
    "    print(\"Best hyperparameters:\")\n",
    "    print(best)\n",
    "\n",
    "    #values are tipically stored in floats; change to ints if required\n",
    "    if just_ints:\n",
    "        integer_best = {key: int(value) for key, value in best.items()}\n",
    "        return integer_best\n",
    "    return best #returns best hyperparameter combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626f9246-c6e8-4feb-bda1-f6b29114f018",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_class_proportions(labels):\n",
    "    \"\"\"\n",
    "    Given an array with the labels (ints), returns an array with the proportion of every class\n",
    "    \"\"\"\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    total_samples = len(labels)\n",
    "    proportions = counts / total_samples\n",
    "    return proportions\n",
    "\n",
    "def kl_divergence(p, q):\n",
    "    \"\"\"\n",
    "    Calculates Kullback-Leibler (KL) divergence between two distribution samples\n",
    "    \"\"\"\n",
    "    return np.sum(np.where(p != 0, p * np.log(p / q), 0))\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################\n",
    "\n",
    "def sgkf_to_holdout(sgkf,X,y,groups):\n",
    "    '''\n",
    "    Takes in a stratified grouped k-fold CV object, and from those k options, finds the best train-test split (with one fold going into testing and the remaining k-1 to training),\n",
    "    that is, the split whose class proportions mostly resembles the global class proportions.\n",
    "    Returns: the integer of the sgkf.split() iteration that matches this best split\n",
    "    '''\n",
    "    \n",
    "    class_proportions = []\n",
    "\n",
    "    #perform every possible split and calculate class proportions in each iteration\n",
    "    for train_index, test_index in sgkf.split(X, y, groups):\n",
    "        train_labels = y[train_index]\n",
    "        test_labels = y[test_index]\n",
    "        class_proportion = calculate_class_proportions(test_labels)\n",
    "        class_proportions.append(class_proportion)\n",
    "    #calculate the global class proportions\n",
    "    global_class_proportions = calculate_class_proportions(y)\n",
    "    #calculate Kullback-Leibler (KL) divergence and find the fold with the closest class proportions\n",
    "    min_kl_divergence = float('inf')\n",
    "    selected_fold = None\n",
    "    #select split with smaller divergence to the global class proportion distribution\n",
    "    for i, proportions in enumerate(class_proportions):\n",
    "        kl_distance = kl_divergence(global_class_proportions, proportions)\n",
    "        if kl_distance < min_kl_divergence:\n",
    "            min_kl_divergence = kl_distance\n",
    "            selected_fold = i\n",
    "    \n",
    "    return selected_fold\n",
    "    \n",
    "########################################################################################################\n",
    "\n",
    "def multipleHoldouts(X,y,groups,n_iter=10):\n",
    "    '''\n",
    "    calls sgkf_to_holdout multiple times, using different sgkf states, to find different hypothesis of grouped stratified holdout.\n",
    "    Returns: list of integers that describe the best iteration of sgkf.split for each sgkf\n",
    "    '''\n",
    "    best_iterations = []\n",
    "    for i in range(n_iter):\n",
    "        sgkf = StratifiedGroupKFold(n_splits=5,shuffle=True,random_state=i)\n",
    "        best_iterations.append(sgkf_to_holdout(sgkf,X,y,groups))\n",
    "    return best_iterations\n",
    "\n",
    "########################################################################################################\n",
    "\n",
    "def performanceSGH(model, X, y, groups, class_names, n_iter=10, plot_cm=True, prints=True, X_added_al=None, y_added_al=None):\n",
    "    '''\n",
    "    Evaluates perfomance for a certain sklearn model, with the stratified grouped holdout procedure.\n",
    "    Outputs results dictionary. Optionally plots confusion matrix\n",
    "    '''\n",
    "    best_iterations = multipleHoldouts(X,y,groups,n_iter)\n",
    "    unique_labels, label_counts = np.unique(y, return_counts=True)\n",
    "    \n",
    "\n",
    "    ovacc_train = []\n",
    "    balacc_train = []\n",
    "    recall_train = []\n",
    "    f1_train = []\n",
    "    \n",
    "    ovacc_test = []\n",
    "    balacc_test = []\n",
    "    recall_test = []\n",
    "    f1_test = []\n",
    "    \n",
    "    for i,best_iter in enumerate(best_iterations):\n",
    "        sgkf = StratifiedGroupKFold(n_splits=5,shuffle=True,random_state=i)\n",
    "        #use the selected fold as test set\n",
    "        test_fold = list(sgkf.split(X, y, groups))[best_iter]\n",
    "        train_index, test_index = test_fold\n",
    "        xx_train, xx_val, yy_train, yy_val = X.loc[train_index], X.loc[test_index], y.loc[train_index], y.loc[test_index] #substituted .values with .loc\n",
    "        clusters_train = groups[train_index]\n",
    "        clusters_test = groups[test_index]\n",
    "        \n",
    "        if X_added_al is not None:\n",
    "            xx_train = pd.concat([xx_train, X_added_al],ignore_index=True)\n",
    "            yy_train = pd.concat([yy_train,y_added_al], ignore_index=True)\n",
    "\n",
    "        if prints:\n",
    "            print(\"Iteration \" + str(i) + \":\")\n",
    "    \n",
    "        model.fit(xx_train,yy_train)\n",
    "\n",
    "        if np.unique(yy_train).shape[0] == 2:\n",
    "            labels_cm = np.array([0,1])\n",
    "            labels_recall = [1]\n",
    "        else:\n",
    "            labels_cm = np.arange(1,8)\n",
    "            labels_recall = [5]\n",
    "        \n",
    "        #train results\n",
    "        preds_train = model.predict(xx_train)\n",
    "        \n",
    "        ovacc_train.append(np.round(model.score(xx_train,yy_train),3))\n",
    "        balacc_train.append(np.round(balanced_accuracy_scorer(model, xx_train, yy_train),3))\n",
    "        recall_train.append(np.round( metrics.recall_score(yy_train, preds_train, labels=labels_recall, average='micro'), 3))\n",
    "        f1_train.append(np.round(f1_cashew_scorer(model,xx_train,yy_train),3))\n",
    "\n",
    "        if prints:\n",
    "            print(\"Ov. Accuracy train: \", ovacc_train[-1]) #accuracy\n",
    "            print(\"Balanced Accuracy train: \", balacc_train[-1])\n",
    "            print(\"Recall cashew train: \", recall_train[-1])\n",
    "            print(\"F1-score cashew train: \", f1_train[-1])\n",
    "            print(\"\\n\")\n",
    "        \n",
    "        #test results\n",
    "        preds = model.predict(xx_val)\n",
    "    \n",
    "        ovacc_test.append(np.round(model.score(xx_val,yy_val),3))\n",
    "        balacc_test.append(np.round(balanced_accuracy_scorer(model, xx_val, yy_val),3))\n",
    "        recall_test.append(np.round( metrics.recall_score(yy_val, preds, labels=labels_recall, average='micro'), 3))\n",
    "        f1_test.append(np.round(f1_cashew_scorer(model, xx_val, yy_val),3))\n",
    "\n",
    "        if prints:\n",
    "            print(\"Ov. Accuracy test: \", ovacc_test[-1]) #accuracy\n",
    "            print(\"Balanced Accuracy test: \", balacc_test[-1])\n",
    "            print(\"Recall cashew test: \", recall_test[-1])\n",
    "            print(\"F1-score cashew test: \", f1_test[-1])\n",
    "            print(\"\\n\")\n",
    "            \n",
    "        #plot confusion matrix\n",
    "        if plot_cm:\n",
    "            cm = metrics.confusion_matrix(yy_val,preds,labels=labels_cm)\n",
    "            cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = cm,display_labels=np.array(class_names))#, display_labels = [\"Morreu\",\"Sobreviveu\"])\n",
    "            cm_display.plot()\n",
    "            plt.xticks(rotation = 45)\n",
    "            plt.show()\n",
    "        \n",
    "    #show tabulates\n",
    "    if prints:\n",
    "        headers=[\"Ov. Accuracy\", \"Bal. Accuracy\", \"Recall Cashew\", \"F1-score Cashew\"]\n",
    "        print(\"Train \\n\")\n",
    "        print(tabulate(np.round( np.array([ovacc_train, balacc_train, recall_train, f1_train]).transpose() ,3), headers=headers))\n",
    "        print(\"\\n Test \\n\")\n",
    "        print(tabulate(np.round( np.array([ovacc_test, balacc_test, recall_test, f1_test]).transpose() ,3), headers=headers))\n",
    "\n",
    "    final_dict = {\"ovacc_train\": ovacc_train, \"balacc_train\": balacc_train, \"recall_train\": recall_train, \"f1_train\": f1_train,\n",
    "                  \"ovacc_test\": ovacc_test, \"balacc_test\": balacc_test, \"recall_test\": recall_test, \"f1_test\": f1_test\n",
    "                 }\n",
    "    return final_dict\n",
    "\n",
    "sgkf = StratifiedGroupKFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b6f522-20de-49da-8faf-d64553b251d8",
   "metadata": {},
   "source": [
    "______________\n",
    "\n",
    "# Download datasets\n",
    "\n",
    "[Back to top.](#Top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489fc75d-2ec0-45b7-aa7e-73caf116445c",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge = os.path.join(base_directory, \"\") #full random sampling dataset\n",
    "merged_train = os.path.join(base_directory, \"\") #RS train set\n",
    "merged_test = os.path.join(base_directory, \"\") #RS test set\n",
    "\n",
    "#entire RS dataset\n",
    "\n",
    "X = merge.drop(columns=[\"C_ID_1\", \"x\", \"y\", \"groupID\"]) #C_ID_1 equivalent to Class; groupID equivalent to polygonID\n",
    "y = merge[\"C_ID_1\"]\n",
    "groups = merge[\"groupID\"]\n",
    "\n",
    "y_update = pd.Series(y == 5, dtype=\"int\")\n",
    "y_update.loc[y_update==0] = 2\n",
    "y_update\n",
    "y = y_update\n",
    "\n",
    "#train and test sets\n",
    "\n",
    "X_train = merged_train.drop(columns=[\"C_ID_1\",\"x\",\"y\"])\n",
    "y_train = merged_train[\"C_ID_1\"]\n",
    "X_test = merged_test.drop(columns=[\"C_ID_1\",\"x\",\"y\"])\n",
    "y_test = merged_test[\"C_ID_1\"]\n",
    "\n",
    "y_train_update = pd.Series(y_train == 5, dtype=\"int\")\n",
    "y_train_update.loc[y_train_update==0] = 2\n",
    "y_train = y_train_update\n",
    "\n",
    "y_test_update = pd.Series(y_test == 5, dtype=\"int\")\n",
    "y_test_update.loc[y_test_update==0] = 2\n",
    "y_test_update\n",
    "y_test = y_test_update\n",
    "\n",
    "#for visualization\n",
    "class_names = [\"Cashew\", \"Non-Cashew\"]\n",
    "labels_cm = np.arange(1,3)\n",
    "labels_recall = [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a369d3e5-ae3d-4b97-954c-cb88abcf5235",
   "metadata": {},
   "source": [
    "_____\n",
    "\n",
    "# Feature selection\n",
    "\n",
    "[Back to Top](#Top)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590cf2a0-a59e-4b49-9503-c2c254e3761d",
   "metadata": {},
   "source": [
    "________\n",
    "\n",
    "## __Above average threshold cut__\n",
    "\n",
    "Basically, use previous best RF and train it on the Random Sampling train set. Get feature_importances_ and their average value. Select only the features with _feature_importances__ above the average value.\n",
    "\n",
    "[Back to Top](#Top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c540ea7-f1f0-40b7-9f88-9688f0b06833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train previous best model\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=98, max_depth=8, max_features=19, random_state=42) #previous best model\n",
    "rf.fit(X_train,y_train)\n",
    "\n",
    "#train results\n",
    "preds_train = rf.predict(X_train)\n",
    "print(\"Accuracy train: \", np.round(rf.score(X_train,y_train),3)) #accuracy\n",
    "print(\"Recall cashew train: \", np.round( metrics.recall_score(y_train, preds_train, labels=labels_recall, average='micro'), 3) )\n",
    "print(\"F1-score train: \", np.round(f1_score(y_train,preds_train,average=None),3))\n",
    "print(\"\\n\")\n",
    "cm = metrics.confusion_matrix(y_train,preds_train,labels=labels_cm)\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = cm,display_labels=np.array(class_names))\n",
    "cm_display.plot()\n",
    "plt.xticks(rotation = 45)\n",
    "plt.show()\n",
    "\n",
    "#test results\n",
    "preds = rf.predict(X_test)\n",
    "print(\"Accuracy test: \", np.round(rf.score(X_test,y_test),3)) #accuracy\n",
    "print(\"Recall cashew test: \", np.round( metrics.recall_score(y_test, preds, labels=labels_recall, average='micro'), 3) )\n",
    "print(\"F1-score test: \", np.round(f1_score(y_test,preds,average=None),3))\n",
    "cm = metrics.confusion_matrix(y_test,preds,labels=labels_cm)\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = cm,display_labels=np.array(class_names))\n",
    "cm_display.plot()\n",
    "plt.xticks(rotation = 45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27e87af-c9d9-49c9-8459-0a9f9e746999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get feature importances and perform cut based on average value\n",
    "\n",
    "#feature importances\n",
    "feature_importances = rf.feature_importances_\n",
    "feature_names = X_train.columns\n",
    "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "feature_importance_df.reset_index(inplace=True)\n",
    "\n",
    "print(\"Features above 1/n_feat: \", np.unique(feature_importances>1/feature_importances.shape[0], return_counts=True))\n",
    "\n",
    "#visualize ordered feature importances and average value cutoff\n",
    "plt.figure(figsize=(10, 60))\n",
    "plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])\n",
    "plt.axvline(x=1/360, color='r', linestyle='--')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.ylim(-1, 360)\n",
    "plt.title('ALL Feature Importances')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2ec6ea-0f7f-4ef3-835b-beaaec17dc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_cashewnon = get_feature_importances(rf, X_train)\n",
    "\n",
    "plot_n_top_bottom(feature_importance_cashewnon, n=180)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db1f22b-52f9-42c3-9284-b7fddfda36d7",
   "metadata": {},
   "source": [
    "* __Above-average method selects 67 features__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57e41cb-e7e6-4247-9b13-b29c06f455dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#results with 67 features\n",
    "\n",
    "columns = feature_importance_df.loc[feature_importance_df[\"Importance\"] > 1/360][\"Feature\"]\n",
    "print(columns.shape)\n",
    "rf = RandomForestClassifier(n_estimators=98, max_depth=8, max_features=19, random_state=42)\n",
    "just_aboveavg_results = performanceSGH(rf, X[columns], pd.Series(y), groups, class_names, 20, plot_cm=False, prints=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460c5a0a-c45d-4196-8b31-89fba1f7a94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#results with 360 total features\n",
    "\n",
    "columns = X.columns\n",
    "print(columns.shape)\n",
    "rf = RandomForestClassifier(n_estimators=98, max_depth=8, max_features=19, random_state=42)\n",
    "just_every_results = performanceSGH(rf, X[columns], pd.Series(y), groups, class_names, 20, plot_cm=False, prints=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b87b3b-31bb-45d4-92af-0ff7b83f3092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization of results\n",
    "\n",
    "results = [just_every_results, just_aboveavg_results]\n",
    "results_names = [\"Every feature\", \"Above avg importance\"]\n",
    "\n",
    "keys = list(results[0].keys())\n",
    "\n",
    "plot_cols=2\n",
    "plot_rows=(len(keys)+1)//2\n",
    "plt.figure(figsize=(12,20))\n",
    "\n",
    "for k,key in enumerate(keys):\n",
    "    data = []\n",
    "    for r, typeof_result in enumerate(results):\n",
    "        data.append(typeof_result[key])\n",
    "    plt.subplot(plot_rows,plot_cols,k+1)\n",
    "    plt.boxplot(data, labels = results_names)\n",
    "    plt.title(key)\n",
    "    plt.title(key)\n",
    "plt.tight_layout()    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74609ed5-5aa0-401c-94a0-0b010aebd0e6",
   "metadata": {},
   "source": [
    "_________________________\n",
    "\n",
    "# RFECV cut\n",
    "\n",
    "Performed with sklearn's _Recursive Feature Elimination through Cross Validation_ method.\n",
    "\n",
    "[Back to Top](#Top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722b9d36-6e9c-4b46-9b09-d3385cb2a6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def scorer(estimator, X_true, y_true):\n",
    "    y_pred = estimator.predict(X_true)\n",
    "    print(np.unique(y_pred))\n",
    "    print(np.unique(y_true))\n",
    "    return f1_score(y_true,y_pred,average=None)[4] #gets f1-score for cashew\n",
    "\n",
    "estimator = RandomForestClassifier(n_estimators=98, max_depth=8, max_features=19, random_state=42) #use previous best model\n",
    "selector = RFECV(estimator, step=10, cv=5, scoring=scorer, verbose=1, n_jobs=3) #recursive feature elimination method\n",
    "selector = selector.fit(X_train, y_train)\n",
    "print(selector.support_)\n",
    "print(selector.ranking_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a5dc03-2a49-48ae-a442-b4164e61dce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize RFECV results\n",
    "\n",
    "n_scores = len(selector.cv_results_[\"mean_test_score\"])\n",
    "min_features_to_select=1\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Mean test accuracy\")\n",
    "plt.errorbar(\n",
    "    range(min_features_to_select, n_scores + min_features_to_select),\n",
    "    selector.cv_results_[\"mean_test_score\"],\n",
    "    yerr=selector.cv_results_[\"std_test_score\"],\n",
    ")\n",
    "plt.title(\"Recursive Feature Elimination \\nwith correlated features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb3c381-bb59-4f8f-87a0-cefb3c8d0a37",
   "metadata": {},
   "source": [
    "__________\n",
    "\n",
    "# Hyperparameter tuning\n",
    "\n",
    "[Back to Top](#Top)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e3a685-df30-4c48-b679-fe8efb8d18c0",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## HP for Random Forests\n",
    "\n",
    "[Back to Top](#Top)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd5911e-436f-4468-a999-85e471e8e79f",
   "metadata": {},
   "source": [
    "* __Every feature__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213bdf96-47a9-419b-842f-5ee610105d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = feature_importance_df[\"Feature\"]\n",
    "space = {\n",
    "    'n_estimators': hp.uniformint(\"n_estimators\", low=50, high=300, q=10),\n",
    "    'max_depth': hp.uniformint(\"max_depth\", low=1, high=15, q=3), \n",
    "    'max_features': hp.uniformint(\"max_features\", low=2, high=columns.shape[0], q=7)\n",
    "}\n",
    "best_hp_every_train = hyperopt_get_best(X_train[columns], y_train, f1_cashew_scorer, max_evals=60, timeout=7200, space=space, just_ints=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f16b74-1314-423e-b76f-4915420f43d1",
   "metadata": {},
   "source": [
    "* __Above average cutoff__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58116ec7-696f-4900-aee5-98596fe903fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = feature_importance_df.loc[feature_importance_df[\"Importance\"] > 1/360][\"Feature\"]\n",
    "space = {\n",
    "    'n_estimators': hp.uniformint(\"n_estimators\", low=50, high=300, q=10),\n",
    "    'max_depth': hp.uniformint(\"max_depth\", low=1, high=15, q=3), \n",
    "    'max_features': hp.uniformint(\"max_features\", low=2, high=columns.shape[0], q=7)\n",
    "}\n",
    "best_hp_aboveavg_train = hyperopt_get_best(X_train[columns], y_train, f1_cashew_scorer, max_evals=60, timeout=7200, space=space, just_ints=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57422cef-2510-4425-8a12-814f8940b9f7",
   "metadata": {},
   "source": [
    "* __With RFECV cutoff__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60eef925-2c69-4419-ba29-725109216dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names_rfecv\n",
    "X_train_cut_rfecv = X_train[feature_names_rfecv]\n",
    "X_test_cut_rfecv = X_test[feature_names_rfecv]\n",
    "\n",
    "max_evals = 60\n",
    "timeout = 12600\n",
    "best_hp_rfecv = hyperopt_get_best(X_train_cut_rfecv,y_train, max_evals = max_evals, timeout = timeout, space=space) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dada2fd2-7bd8-4554-92db-b7bd311918c8",
   "metadata": {},
   "source": [
    "______\n",
    "\n",
    "# HP for SVMs\n",
    "\n",
    "[Back to Top](#Top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bec9c8-59b7-4dd5-b656-05812857d38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVMs work much better with normalized data\n",
    "\n",
    "X_rs_norm = get_normalized_X(X, norm)\n",
    "X_rs_train_norm = get_normalized_X(X_train, norm)\n",
    "X_rs_test_norm = get_normalized_X(X_test, norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725bce4d-c379-4d27-b7f0-22ff46c18727",
   "metadata": {},
   "source": [
    "* __Every feature__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1576d1ac-4000-46c5-bc72-c26ba76adfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = feature_importance_df[\"Feature\"]\n",
    "space = {\n",
    "    'C': hp.loguniform(\"C\", low=np.log(1e-3), high=np.log(1e1)),\n",
    "    'gamma': hp.loguniform(\"gamma\", low=np.log(1e-10), high=np.log(1)),\n",
    "    'kernel': hp.choice('kernel',['linear', 'poly', 'rbf']),\n",
    "    'degree':hp.choice('degree',[2,3,4]),\n",
    "    'class_weight': hp.choice('class_weight', [None, \"balanced\"])\n",
    "}\n",
    "best_hp_every_train = hyperopt_get_best(X_rs_train_norm[columns], y_train, f1_cashew_scorer, max_evals=60, timeout=7200, space=space, just_ints=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09337c03-3d8d-4c58-ad06-c31cd38878a8",
   "metadata": {},
   "source": [
    "* __Above average cutoff__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a505dd-3a4d-4088-9b97-4ff0de30a586",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = feature_importance_df.loc[feature_importance_df[\"Importance\"] > 1/360][\"Feature\"]\n",
    "space = {\n",
    "    'C': hp.loguniform(\"C\", low=np.log(1e-3), high=np.log(1e1)),\n",
    "    'gamma': hp.loguniform(\"gamma\", low=np.log(1e-10), high=np.log(1)),\n",
    "    'kernel': hp.choice('kernel',['linear', 'poly', 'rbf']),\n",
    "    'degree':hp.choice('degree',[2,3,4]),\n",
    "    'class_weight': hp.choice('class_weight', [None, \"balanced\"])\n",
    "}\n",
    "best_hp_aboveavg_train = hyperopt_get_best(X_rs_train_norm[columns], y_train, f1_cashew_scorer, max_evals=60, timeout=7200, space=space, just_ints=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6da7904-78cd-4e02-b67e-648f88d1f88b",
   "metadata": {},
   "source": [
    "* __With RFECV cutoffs__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dbdbd4-bcf1-4bf2-9053-f03ebaac3796",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names_rfecv\n",
    "X_train_cut_rfecv = X_rs_train_norm[feature_names_rfecv]\n",
    "X_test_cut_rfecv = X_rs_test_norm[feature_names_rfecv]\n",
    "\n",
    "max_evals = 60\n",
    "timeout = 12600\n",
    "best_hp_rfecv = hyperopt_get_best(X_train_cut_rfecv,y_train, max_evals = max_evals, timeout = timeout, space=space) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4efb187-d65a-4bb1-a9bd-562d3864f5a9",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "# Bottom\n",
    "\n",
    "[Back to top](#Top)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
